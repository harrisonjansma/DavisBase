{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-12-03T21:10:47.503Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-2-c203a48cda15>, line 1274)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-c203a48cda15>\"\u001b[1;36m, line \u001b[1;32m1274\u001b[0m\n\u001b[1;33m    cells2copy.append(cells[i]['cell_binary'])\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import struct\n",
    "import sys\n",
    "from datetime import datetime, time\n",
    "import re\n",
    "import pdb\n",
    "\n",
    "############################################################################\n",
    "\n",
    "def check_input(command):\n",
    "    if len(command)==0:\n",
    "        pass\n",
    "\n",
    "    elif command[-1]!=\";\":\n",
    "        print(\"All commands end with semicolon.\")\n",
    "\n",
    "    elif command == \"help;\":\n",
    "        help()\n",
    "\n",
    "    elif command == \"show tables;\":\n",
    "        show_tables()\n",
    "\n",
    "    elif command[0:len(\"create table \")] == \"create table \":\n",
    "        create_table(command)\n",
    "\n",
    "    elif command[0:len(\"drop table \")] == \"drop table \":\n",
    "        drop_table(command)\n",
    "\n",
    "    elif command[0:len(\"create index \")] == \"create index \":\n",
    "        create_index(command)\n",
    "\n",
    "    elif command[0:len(\"insert \")] == \"insert \":\n",
    "        insert_into(command)\n",
    "\n",
    "    elif command[0:len(\"delete \")] == \"delete \":\n",
    "        insert_into(command)\n",
    "\n",
    "    elif command[0:len(\"update \")] == \"update \":\n",
    "        insert_into(command)\n",
    "\n",
    "    elif command[0:len(\"select \")] == \"select \":\n",
    "        query(command)\n",
    "\n",
    "    elif command == \"exit;\":\n",
    "        return True\n",
    "\n",
    "    elif command == \"test;\":\n",
    "        return True\n",
    "\n",
    "    else:\n",
    "        print(\"Command \\\"{}\\\" not recognized\".format(command))\n",
    "\n",
    "####################################################################\n",
    "# COMPLETED FUNCTIONS\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def init():\n",
    "    if os.path.exists('davisbase_columns.tbl'):\n",
    "        pass\n",
    "    else:\n",
    "        initialize_file('davisbase_columns', True)\n",
    "        file_name = \"davisbase_columns.tbl\"\n",
    "        davisbase_columns_schema = ['TEXT', 'TEXT', 'TEXT', 'TINYINT', 'TEXT', 'TEXT', 'TEXT']\n",
    "\n",
    "        davisbase_columns_cells = [[\"davisbase_tables\", \"rowid\", \"INT\", 1, \"NO\", 'NO', 'NO' ],\n",
    "                [\"davisbase_tables\", \"table_name\", \"TEXT\", 2, \"NO\", 'NO', 'NO' ],\n",
    "                 [\"davisbase_columns\", \"rowid\", \"INT\", 1, \"NO\", 'NO', 'NO' ],\n",
    "                [\"davisbase_columns\", \"table_name\", \"TEXT\", 2, \"NO\", 'NO', 'NO' ],\n",
    "                [\"davisbase_columns\", \"column_name\", \"TEXT\", 3, \"NO\", 'NO', 'NO' ],\n",
    "                [\"davisbase_columns\", \"data_type\", \"TEXT\", 4, \"NO\", 'NO', 'NO' ],\n",
    "                [\"davisbase_columns\", \"ordinal_position\", \"TINYINT\", 5, \"NO\", 'NO', 'NO' ],\n",
    "                [\"davisbase_columns\", \"is_nullable\", \"TEXT\", 6, \"NO\", 'NO', 'NO' ],\n",
    "              [\"davisbase_columns\", \"unique\", \"TEXT\", 7, \"NO\", 'NO', 'NO' ],\n",
    "              [\"davisbase_columns\", \"primary_key\", \"TEXT\", 8, \"NO\", 'NO', 'NO' ]]\n",
    "\n",
    "        for i, cell in enumerate(davisbase_columns_cells):\n",
    "            cell = table_create_cell(davisbase_columns_schema, cell, False, left_child_page=None,  rowid=i+1)\n",
    "            try:#cant use insert, because insert requires use of catalog, must do this first one manually\n",
    "                page_insert_cell(file_name, 0, cell)\n",
    "            except:\n",
    "                table_leaf_split_page(file_name, 0, cell)\n",
    "\n",
    "    if os.path.exists('davisbase_tables.tbl'):\n",
    "        pass\n",
    "    else:\n",
    "        initialize_file('davisbase_tables', True)\n",
    "        file_name = \"davisbase_tables.tbl\"\n",
    "        davisbase_tables_schema = ['TEXT']\n",
    "\n",
    "        cells = [[\"davisbase_tables\"],\n",
    "                [\"davisbase_columns\"]]\n",
    "        for i, cell in enumerate(cells):\n",
    "            cell = table_create_cell(davisbase_tables_schema, cell, False, left_child_page=None,  rowid=i+1)\n",
    "            try:\n",
    "                page_insert_cell(file_name, 0, cell)\n",
    "            except:\n",
    "                print(\"cell_size:\",len(cell))\n",
    "                file_bytes = load_file(file_name)\n",
    "                print(\"Remaining space in page:\", page_available_bytes(file_bytes, 0))\n",
    "\n",
    "\n",
    "\n",
    "def help():\n",
    "    print(\"DavisBase supported commands.\")\n",
    "    print(\"##########################################\")\n",
    "    print(\"SHOW TABLES;\")\n",
    "    print(\"CREATE TABLE ...;\")\n",
    "    print(\"DROP TABLE ...;\")\n",
    "    print(\"CREATE INDEX ...;\")\n",
    "    print(\"INSERT INTO ...;\")\n",
    "    print(\"DELETE FROM ...;\")\n",
    "    print(\"UPDATE ...\")\n",
    "    print(\"SELECT ...;\")\n",
    "    print(\"EXIT;\")\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def initialize_file(table_name, is_table):\n",
    "    \"\"\"Creates a file and writes the first, empty page (root)\"\"\"\n",
    "    if is_table:\n",
    "        file_type = \".tbl\"\n",
    "    else:\n",
    "        file_type = '.ndx'\n",
    "    if os.path.exists(table_name+file_type):\n",
    "        os.remove(table_name+file_type)\n",
    "    with open(table_name+file_type, 'w+') as f:\n",
    "        pass\n",
    "    write_new_page(table_name, is_table, False, 0, -1)\n",
    "    return None\n",
    "\n",
    "\n",
    "def initialize_indexes(column_dictionary):\n",
    "    \"\"\"\n",
    "    dictionary = {\n",
    "    'table_name':{\n",
    "        \"column1\":{\n",
    "            'data_type':\"int\",\n",
    "            'ordinal_position':1,\n",
    "            'is_nullable':'YES',\n",
    "            'unique':'NO'\n",
    "            'primary_key':'YES'\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \"\"\"\n",
    "    table = list(column_dictionary.keys())\n",
    "    table_name = table[0]\n",
    "    column_names = list(column_dictionary[table_name].keys())\n",
    "    columns = list(column_dictionary[table_name].values())\n",
    "\n",
    "    for col in column_names:\n",
    "        if column_dictionary[table_name][col]['primary_key']=='YES':\n",
    "            index_name = table_name+'_'+col\n",
    "            initialize_file(index_name, False) #create the empty ndx file for primary key\n",
    "    return None\n",
    "\n",
    "\n",
    "def catalog_add_table(column_dictionary):\n",
    "    \"\"\"\n",
    "    dictionary = {\n",
    "    'table_name':{\n",
    "        \"column1\":{\n",
    "            'data_type':\"int\",\n",
    "            'ordinal_position':1,\n",
    "            'is_nullable':'YES',\n",
    "            'unique':'NO'\n",
    "            'primary_key':'YES'\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \"\"\"\n",
    "    table = list(column_dictionary.keys())\n",
    "    assert(len(table)==1)\n",
    "    table_name = table[0]\n",
    "    columns =  column_dictionary[table_name]\n",
    "    column_names = list(column_dictionary[table_name].keys())\n",
    "    insert(\"davisbase_tables\", [table_name])\n",
    "    insert(\"davisbase_columns\",[table_name, \"rowid\", \"INT\", 1, \"NO\", 'NO', 'NO' ] )\n",
    "    for col in column_names:\n",
    "        values=[table_name, col, columns[col]['data_type'].upper(), columns[col]['ordinal_position']+1, columns[col]['is_nullable'].upper(), columns[col]['unique'].upper(), columns[col]['primary_key'].upper()]\n",
    "        insert(\"davisbase_columns\", values)\n",
    "\n",
    "\n",
    "\n",
    "def write_new_page(table_name, is_table, is_interior, rsibling_rchild, parent):\n",
    "    \"\"\"Writes a empty page to the end of the file with an appropriate header for the kind of table/index\"\"\"\n",
    "    assert(type(is_table)==bool)\n",
    "    assert(type(is_interior)==bool)\n",
    "    assert(type(rsibling_rchild)==int)\n",
    "    assert(type(parent)==int)\n",
    "    is_leaf = not is_interior\n",
    "    is_index = not is_table\n",
    "    if is_table:\n",
    "        file_type = \".tbl\"\n",
    "    else:\n",
    "        file_type = '.ndx'\n",
    "\n",
    "    file_size = os.path.getsize(table_name + file_type)\n",
    "    with open(table_name + file_type, 'ab') as f:\n",
    "        newpage = bytearray(PAGE_SIZE*b'\\x00')\n",
    "        #first byte says what kind of page it is\n",
    "        if is_table and is_interior:\n",
    "            newpage[0:1] = b'\\x05'\n",
    "        elif is_table and is_leaf:\n",
    "            newpage[0:1] = b'\\x0d'\n",
    "        elif is_index and is_interior:\n",
    "            newpage[0:1] = b'\\x02'\n",
    "        elif is_index and is_leaf:\n",
    "            newpage[0:1] = b'\\x0a'\n",
    "        else:\n",
    "             raise ValueError(\"Page must be table/index\")\n",
    "        newpage[2:16] = struct.pack(endian+'hhii2x', 0, PAGE_SIZE, rsibling_rchild, parent)\n",
    "        f.write(newpage)\n",
    "        assert(file_size%PAGE_SIZE==0)\n",
    "        return int(file_size/PAGE_SIZE)\n",
    "\n",
    "\n",
    "\n",
    "def dtype_to_int(dtype):\n",
    "    \"\"\"based on the documentation, each dtype has a single-digit integer encoding\"\"\"\n",
    "    dtype = dtype.lower()\n",
    "    mapping = {\"null\":0,\"tinyint\":1, \"smallint\":2, \"int\":3, \"bigint\":4, \"long\":4, 'float':5, \"double\":6, \"year\":8, \"time\":9, \"datetime\":10, \"date\":11, \"text\":12}\n",
    "    return mapping[dtype]\n",
    "\n",
    "\n",
    "def int_to_fstring(key):\n",
    "    \"\"\"format string for use in struct.pack/struct.unpack\"\"\"\n",
    "    int2packstring={\n",
    "    2:'h', 3:'i', 4:'q', 5:'f', 6:'d',\n",
    "    9:'i', 10:'Q', 11:'Q' }\n",
    "    return int2packstring[key]\n",
    "\n",
    "\n",
    "def schema_to_int(schema, values):\n",
    "    \"\"\"given a list of data types ex [int, year] ,convert to single-digit integer appropriate.\"\"\"\n",
    "    dtypes = [dtype_to_int(dt) for dt in schema]\n",
    "    for i, val in enumerate(values):\n",
    "        if val==None: #regardless of the col dtype, if null-> dt = 0\n",
    "            dtypes[i]=0\n",
    "            continue\n",
    "        elif dtypes[i]==12: #add the len of the string to dtype\n",
    "            dtypes[i]+=len(val)\n",
    "    return dtypes\n",
    "\n",
    "\n",
    "def get_dt_size(dt):\n",
    "    \"\"\"given the single-digit encoding for data type return the number of bytes this data takes\"\"\"\n",
    "    if dt==0:\n",
    "        return 0\n",
    "    if dt in [1,8]:\n",
    "        return 1\n",
    "    elif dt in [2]:\n",
    "        return 2\n",
    "    elif dt in [3,5,9]:\n",
    "        return 4\n",
    "    elif dt in [4,6,10,11]:\n",
    "        return 8\n",
    "    elif dt>=12:\n",
    "        return dt-12\n",
    "    else:\n",
    "        raise ValueError(\"what happened????\")\n",
    "\n",
    "\n",
    "def date_to_bytes(date, time=False):\n",
    "    if not time:\n",
    "        return struct.pack(\">q\", int(round(date.timestamp() * 1000)))\n",
    "    else:\n",
    "        return struct.pack(\">i\", int(round(date.timestamp() * 1000)))\n",
    "\n",
    "\n",
    "def bytes_to_dates(bt, time=False):\n",
    "    if not time:\n",
    "        return datetime.fromtimestamp((struct.unpack(\">q\", bt)[0])/1000)\n",
    "    else:\n",
    "        return datetime.fromtimestamp((struct.unpack(\">i\", bt)[0])/1000)\n",
    "\n",
    "\n",
    "def time_to_byte(t):\n",
    "    d =  datetime(1970,1,2,t.hour,t.minute, t.microsecond)\n",
    "    return date_to_bytes(d, time=True)\n",
    "\n",
    "\n",
    "def byte_to_time(bt):\n",
    "    return bytes_to_dates(bt, time=True).time()\n",
    "\n",
    "\n",
    "def val_dtype_to_byte(val, dt):\n",
    "    \"\"\"given a value and a single-digit dtype rep, covert to binary string\"\"\"\n",
    "    if val == None: #NULL\n",
    "        return b''\n",
    "    if dt==1: #one byte int\n",
    "        return val.to_bytes(1, byteorder=sys.byteorder, signed=True)\n",
    "    if dt==8: #one byte year relative to 2000\n",
    "        return (val-2000).to_bytes(1, byteorder=sys.byteorder, signed=True)\n",
    "    if dt in [2,3,4,5,6]: #alldtypes i can convert with struct object\n",
    "        return struct.pack(int_to_fstring(dt), val)\n",
    "    if dt in [10,11]: #datetime, date objects\n",
    "        return date_to_bytes(val)\n",
    "    if dt==9: #time object\n",
    "        return time_to_byte(val)\n",
    "    elif dt>=12:  #look for text\n",
    "        return val.encode('ascii')\n",
    "\n",
    "\n",
    "def dtype_byte_to_val(dt, byte_str):\n",
    "    \"\"\"Given the single-digit dtype encoding and byte string of approp size, returns Python value\"\"\"\n",
    "    if dt==0:  #null type\n",
    "        return None\n",
    "    elif dt==1: #onebyteint\n",
    "        return int.from_bytes(byte_str, byteorder=sys.byteorder, signed=False)\n",
    "    elif dt==8: #one byte year\n",
    "        return int.from_bytes(byte_str, byteorder=sys.byteorder, signed=False)+2000\n",
    "    elif dt in [2,3,4,5,6]: #alldtypes i can convert with struct object\n",
    "        return struct.unpack(int_to_fstring(dt), byte_str)[0]\n",
    "    if dt in [10,11]: #datetime, dateobjects\n",
    "        return bytes_to_dates(byte_str)\n",
    "    if dt==9:#time\n",
    "        return byte_to_time(byte_str)\n",
    "    elif dt>=12:  #text\n",
    "        return byte_str.decode(\"utf-8\")\n",
    "    else:\n",
    "         raise ValueError(\"dtype_byte_to_val????\")\n",
    "\n",
    "\n",
    "def table_values_to_payload(schema, value_list):\n",
    "    \"\"\"given a list of database string formatted datatypes ['int'] and an assoc\n",
    "    list of values with NULL=None\n",
    "\n",
    "    returns a bytestring of all elements in value_list and a single-digit repr of the data types\"\"\"\n",
    "    dtypes = schema_to_int(schema, value_list)\n",
    "    byte_string = b''\n",
    "    for val, dt in zip(value_list, dtypes):\n",
    "        byte_val = val_dtype_to_byte(val, dt)\n",
    "\n",
    "        byte_string += byte_val\n",
    "    return byte_string, dtypes\n",
    "\n",
    "\n",
    "def table_payload_to_values(payload):\n",
    "    \"\"\"\n",
    "    Takes the entire bitstring payload and outputs the values in a list (None=Null)\n",
    "    \"\"\"\n",
    "    num_columns = payload[0]\n",
    "    temp = payload[1:]\n",
    "    dtypes =  temp[:num_columns]\n",
    "    temp = temp[num_columns:]\n",
    "    i = 0\n",
    "    values = []\n",
    "    for dt in dtypes:\n",
    "        element_size = get_dt_size(dt)\n",
    "        byte_str = temp[i:i+element_size]\n",
    "        values.append(dtype_byte_to_val(dt, byte_str))\n",
    "        i+=element_size\n",
    "    assert(i==len(temp))\n",
    "    return values\n",
    "\n",
    "\n",
    "def index_dtype_value_rowids_to_payload(index_dtype, index_value, rowid_list):\n",
    "    \"\"\"\n",
    "    given list of database string dtype reps ['int'] single value of index, and list of integers\n",
    "\n",
    "    returns the bytestring payload for an index cell\n",
    "    \"\"\"\n",
    "    dt = schema_to_int([index_dtype], [index_value])\n",
    "    bin_num_assoc_rowids = bytes([len(rowid_list)])\n",
    "    bin_indx_dtype = bytes(dt)\n",
    "    bin_index_val = val_dtype_to_byte(index_value, *dt)\n",
    "    bin_rowids = struct.pack(endian+str(len(rowid_list))+'i', *rowid_list)\n",
    "    payload = bin_num_assoc_rowids + bin_indx_dtype + bin_index_val+bin_rowids\n",
    "    return payload\n",
    "\n",
    "\n",
    "def index_payload_to_values(payload):\n",
    "    \"\"\"import bytestring payload from index cell outputs the index value and list of rowids\"\"\"\n",
    "    assoc_row_ids = payload[0]\n",
    "    indx_dtype = payload[1]\n",
    "\n",
    "    element_size = get_dt_size(indx_dtype)\n",
    "    indx_byte_str = payload[2:2+element_size]\n",
    "    indx_value = dtype_byte_to_val(indx_dtype, indx_byte_str)\n",
    "\n",
    "    bin_rowid_list  = payload[2+element_size:]\n",
    "\n",
    "    i=0\n",
    "    j = len(bin_rowid_list)\n",
    "    rowid_values = []\n",
    "    while(i<j):\n",
    "        rowid_values.append(struct.unpack(endian+'i', bin_rowid_list[i:i+4])[0])\n",
    "        i+=4\n",
    "\n",
    "    return indx_value, rowid_values\n",
    "\n",
    "\n",
    "def table_create_cell(schema, value_list, is_interior, left_child_page=None,  rowid=None):\n",
    "    \"\"\"\n",
    "    Used to create a cell (binary string representation) that can be inserted into the tbl file\n",
    "\n",
    "    Parameters:\n",
    "    schema (list of strings):  ex. ['int', 'date', 'year']\n",
    "    value_list (list of python values):  ex. [10, '2016-03-23_00:00:00',2004]\n",
    "    is_interior (bool):  is the cell igoing into an interior or leaf page\n",
    "    left_child_page (int):  page_no of left child (only if cell is in interior page).\n",
    "    rowid (int):  rowid of the current cell (only if the cell is going in a leaf page)\n",
    "\n",
    "    Returns:\n",
    "    cell (byte-string): ex. b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00'\n",
    "    \"\"\"\n",
    "    assert(len(value_list)==len(schema))\n",
    "    assert(type(schema)==list)\n",
    "    assert(type(value_list)==list)\n",
    "    assert(type(is_interior)==bool)\n",
    "\n",
    "    if  is_interior:\n",
    "        assert(left_child_page != None)\n",
    "        assert(rowid != None)\n",
    "        cell = struct.pack(endian+'ii', left_child_page, rowid)\n",
    "\n",
    "    else:\n",
    "        assert(rowid != None)\n",
    "        payload_body, dtypes  = table_values_to_payload(schema, value_list)\n",
    "        payload_header = bytes([len(dtypes)]) + bytes(dtypes)\n",
    "        cell_payload = payload_header + payload_body\n",
    "        cell_header = struct.pack(endian+'hi', len(cell_payload), rowid)\n",
    "        cell = cell_header + cell_payload\n",
    "\n",
    "    return cell\n",
    "\n",
    "\n",
    "def index_create_cell(index_dtype, index_value, rowid_list, is_interior, left_child_page=None):\n",
    "    \"\"\"\n",
    "    Used to create a cell (binary string representation) that can be inserted into the ndx file\n",
    "\n",
    "    Parameters:\n",
    "    index_dtype (string): ex\"long\"\n",
    "    index_value (val):  ex' 1037843\n",
    "    rowid_list (list of ints):  [100,22,3214]\n",
    "    is_interior (bool):\n",
    "    left_child_page (int):  only if cell is for interior cell\n",
    "\n",
    "\n",
    "    Returns:\n",
    "    cell (byte-string): ex. b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00'\n",
    "\n",
    "    \"\"\"\n",
    "    assert(type(is_interior)==bool)\n",
    "    is_leaf = not is_interior\n",
    "\n",
    "    payload = index_dtype_value_rowids_to_payload(index_dtype, index_value, rowid_list)\n",
    "    if is_interior:\n",
    "        assert(left_child_page != None)\n",
    "        cell_header = struct.pack(endian+'IH', left_child_page, len(payload))\n",
    "    elif is_leaf:\n",
    "        cell_header = struct.pack(endian+'H', len(payload))\n",
    "    else:\n",
    "         raise ValueError(\"Page must be either table\")\n",
    "\n",
    "    cell = cell_header + payload\n",
    "    return cell\n",
    "\n",
    "\n",
    "def table_read_cell(cell, is_interior):\n",
    "    \"\"\"\n",
    "    Used to read the contents of a cell (byte string)\n",
    "\n",
    "    Parameters:\n",
    "    cell (byte-string): ex b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00'\n",
    "    is_interior (bool):\n",
    "\n",
    "    Returns:\n",
    "    values (dictionary): ex.\n",
    "    interior-> {'left_child_rowid': 1, 'rowid': 10, 'cell_size': 8}\n",
    "    leaf ->{'bytes_in_payload': 61,'num_columns': 10,\n",
    "            'data': [2, 2, 12,10,10, 1.2999999523162842, None,2020, None,10, 10,'hist'],\n",
    "            'cell_size': 67}\n",
    "    \"\"\"\n",
    "    is_leaf = not is_interior\n",
    "\n",
    "    if  is_interior:\n",
    "        cell_header = struct.unpack(endian+'ii', cell[0:8])\n",
    "        res = {'left_child_page':cell_header[0],'rowid':cell_header[1]}\n",
    "    elif is_leaf:\n",
    "        cell_header = struct.unpack(endian+'hi', cell[0:6])\n",
    "        payload = cell[6:]\n",
    "        values = table_payload_to_values(payload)\n",
    "        res = {'bytes':cell_header[0]+6, 'rowid':cell_header[1],\"data\":values}\n",
    "    else:\n",
    "        print(\"error in read cell\")\n",
    "    res[\"cell_size\"]=len(cell)\n",
    "    res['cell_binary'] = cell\n",
    "    return res\n",
    "\n",
    "\n",
    "def index_read_cell(cell, is_interior):\n",
    "    \"\"\"\n",
    "    Used to read the contents of a cell (byte string)\n",
    "\n",
    "    Parameters:\n",
    "    cell (byte-string): ex b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00'\n",
    "    is_interior (bool):\n",
    "\n",
    "    Returns:\n",
    "    values (dictionary):\n",
    "    interior -> {'lchild': 12,'index_value': 1000,'assoc_rowids': [1, 2, 3, 4],'cell_size': 32}\n",
    "    leaf-> {'index_value': 1000, 'assoc_rowids': [1, 2, 3, 4], 'cell_size': 28}\n",
    "    \"\"\"\n",
    "    result=dict()\n",
    "    if  is_interior:\n",
    "        cell_header = struct.unpack(endian+'ih', cell[0:6])\n",
    "        result[\"left_child_page\"]=cell_header[0]\n",
    "        result[\"bytes\"]=cell_header[0]+6\n",
    "        payload = cell[6:]\n",
    "    else:\n",
    "        cell_header = struct.unpack(endian+'h', cell[0:2])\n",
    "        result[\"bytes\"]=cell_header[0]+6\n",
    "        payload = cell[2:]\n",
    "\n",
    "    indx_value, rowid_list = index_payload_to_values(payload)\n",
    "    result[\"index_value\"]=indx_value\n",
    "    result[\"assoc_rowids\"]=rowid_list\n",
    "    result[\"cell_size\"]=len(cell)\n",
    "    result['cell_binary'] = cell\n",
    "    return result\n",
    "\n",
    "\n",
    "def save_page(file_name, page_num, new_page_data):\n",
    "    \"\"\"\n",
    "    Saves the overwrites the page in the file (at loc- page_num) with a byte-string of length PAGE_SIZE\n",
    "\n",
    "    Parameters:\n",
    "    file_name (string): ex 'taco.tbl'\n",
    "    page_num (int): 1\n",
    "    new_page_data(bytestring): b'\\r\\x00\\x07\\x00\\n\\x01\\x00\\x00\\x00\\x00\\xff\\xff\\xff\\xff\\x00\\x00\\xe0\\x01\\xc0\\x01\\xa4\\x01\\x80\\x01\\\\\\x013\\x01\\n\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00'\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    assert(len(new_page_data)==PAGE_SIZE)\n",
    "    file_offset = page_num*PAGE_SIZE\n",
    "    file_offset_end = (page_num+1)*PAGE_SIZE\n",
    "    file_bytes = load_file(file_name)\n",
    "    file_bytes = bytearray(file_bytes)\n",
    "    file_bytes[file_offset:file_offset_end] = new_page_data\n",
    "    with open(file_name, 'r+b') as f:\n",
    "        f.seek(0)\n",
    "        page = f.write(file_bytes)\n",
    "    return None\n",
    "\n",
    "\n",
    "def page_available_bytes(file_bytes, page_num):\n",
    "    page = load_page(file_bytes, page_num)\n",
    "    num_cells = struct.unpack(endian+'h', page[2:4])[0]\n",
    "    bytes_from_top = 16+(2*num_cells)\n",
    "    cell_content_start =struct.unpack(endian+'h', page[4:6])[0]\n",
    "    return  cell_content_start - bytes_from_top\n",
    "\n",
    "\n",
    "def shift_page_content(page, top_indx, bot_indx, shift_step, up=True):\n",
    "    if shift_step==0:\n",
    "        return page\n",
    "    copy = page[top_indx:bot_indx]\n",
    "    if up:\n",
    "        assert(top_indx-shift_step>=0)\n",
    "        new_top_indx = top_indx - shift_step\n",
    "        new_bot_indx = bot_indx - shift_step\n",
    "        page[new_top_indx:new_bot_indx]=copy\n",
    "        page[new_bot_indx:bot_indx]=b'\\x00'*shift_step\n",
    "        return page\n",
    "    else:\n",
    "        assert(bot_indx+shift_step<=PAGE_SIZE)\n",
    "        new_top_indx = top_indx + shift_step\n",
    "        new_bot_indx = bot_indx + shift_step\n",
    "        page[new_top_indx:new_bot_indx]=copy\n",
    "        page[top_indx:new_top_indx]=b'\\x00'*shift_step\n",
    "        return page\n",
    "\n",
    "\n",
    "def update_array_values(page, first_array_loc_to_change, num_cells, shift_step, up=True):\n",
    "    \"\"\"When we shift the page content for cells, the pointers in the page header become incorrect.\"\"\"\n",
    "    if shift_step==0:\n",
    "        return page\n",
    "    if up:\n",
    "        for i in range(first_array_loc_to_change, num_cells):\n",
    "            arr_top = 16+2*i\n",
    "            arr_bot = 16+2*(i+1)\n",
    "            prev_val = struct.unpack(endian+'h',page[arr_top:arr_bot])[0]\n",
    "            page[arr_top:arr_bot]=struct.pack(endian+'h', prev_val-shift_step)\n",
    "    else:\n",
    "        for i in range(first_array_loc_to_change, num_cells):\n",
    "            arr_top = 16+2*i\n",
    "            arr_bot = 16+2*(i+1)\n",
    "            prev_val = struct.unpack(endian+'h',page[arr_top:arr_bot])[0]\n",
    "            page[arr_top:arr_bot]=struct.pack(endian+'h', prev_val+shift_step)\n",
    "    return page\n",
    "\n",
    "\n",
    "def get_cell_indices(page, cell_indx):\n",
    "    cell_top_idx = struct.unpack(endian+'h',page[16+2*cell_indx:16+2*(cell_indx+1)])[0]\n",
    "    if cell_indx==0: #if cell is first on the page (bottom)\n",
    "        cell_bot_idx = PAGE_SIZE\n",
    "    else:\n",
    "        cell_bot_idx = struct.unpack(endian+'h',page[16+2*(cell_indx-1):16+2*(cell_indx)])[0]\n",
    "    return cell_top_idx, cell_bot_idx\n",
    "\n",
    "\n",
    "\n",
    "def page_delete_cell(file_name, page_num, cell_indx):\n",
    "    \"\"\"\n",
    "    Deletes a bytestring into a page from a table or index file. Updates the page header. Fails index given is out of bounds (2, when there is only one cell in page)\n",
    "    Fails if page is empty (no cells). RETURNS IS_EMPTY FLAG (empty after deletion)\n",
    "\n",
    "    Parameters:\n",
    "    file_name (string): ex 'taco.tbl'\n",
    "    page_num (int): 1\n",
    "    cell (byte-string): ex b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00'\n",
    "\n",
    "    Returns:\n",
    "    is_empty (bool): False\n",
    "    \"\"\"\n",
    "    file_bytes = load_file(file_name)\n",
    "    page = load_page(file_bytes, page_num)\n",
    "    page = bytearray(page)\n",
    "    num_cells = struct.unpack(endian+'h', page[2:4])[0]\n",
    "    assert(cell_indx<=num_cells-1)#index starts at 0\n",
    "    assert(num_cells>=1) #delete CAN empty a page\n",
    "    assert(cell_indx>=0)\n",
    "\n",
    "    cell_content_area_start = struct.unpack(endian+'h', page[4:6])[0]\n",
    "    end_of_array = 16+2*num_cells\n",
    "    array_idx_top = 16+2*cell_indx\n",
    "    array_idx_bot = 16+2*(cell_indx+1)\n",
    "\n",
    "    #if cell is the last cell (but not if theres only one cell left)\n",
    "    if (cell_indx==num_cells-1) & (cell_indx!=0):\n",
    "        cell_top_loc, cell_bot_loc = get_cell_indices(page, cell_indx)\n",
    "        cell_2_delete = page[cell_top_loc:cell_bot_loc]\n",
    "        dis2replace= len(cell_2_delete)                    #overwrite the cell2delete\n",
    "        page[cell_top_loc:cell_bot_loc]=b'\\x00'*dis2replace  #change the cell_start area in header\n",
    "        page[4:6] = struct.pack(endian+'h', cell_content_area_start+dis2replace)\n",
    "        #delete last entri in cell array\n",
    "        page[16+2*(num_cells-1):16+2*(num_cells)]=b'\\x00'*2\n",
    "        #update the number of cells\n",
    "        page[2:4] = struct.pack(endian+'h', num_cells-1)\n",
    "\n",
    "    else:\n",
    "        cell_top_loc, cell_bot_loc = get_cell_indices(page, cell_indx)\n",
    "        cell_2_delete = page[cell_top_loc:cell_bot_loc]\n",
    "        dis2replace= len(cell_2_delete)\n",
    "        #shift cell content down\n",
    "        page = shift_page_content(page, cell_content_area_start, cell_top_loc, dis2replace, up=False)\n",
    "        #since we just shifted every cell, every value in cell_array is off\n",
    "        page = update_array_values(page, cell_indx, num_cells, dis2replace, up=False)\n",
    "        #change the cell_start area\n",
    "        page[4:6] = struct.pack(endian+'h', cell_content_area_start+dis2replace)\n",
    "        #shift cell array up (deletes entry for deleted cell)\n",
    "        page = shift_page_content(page, array_idx_bot, end_of_array, 2, up=True)\n",
    "        #update num of cells\n",
    "        page[2:4] = struct.pack(endian+'h', num_cells-1)\n",
    "    save_page(file_name, page_num, page)\n",
    "    assert(len(page)==PAGE_SIZE) #ensure page is same size\n",
    "    return (num_cells - 1) == 0\n",
    "\n",
    "\n",
    "def page_update_cell(file_name, page_num, cell_indx, cell):\n",
    "    \"\"\"\n",
    "    updates a bytestring into a page from a table or index file. Updates the page header. Fails index given is out of bounds (2, when there is only one cell in page)\n",
    "    Fails if page is empty (no cells). RETURNS IS_EMPTY FLAG\n",
    "\n",
    "    Need to think about that happens when an upate cell causes the page to be full\n",
    "\n",
    "    Parameters:\n",
    "    file_name (string): ex 'taco.tbl'\n",
    "    page_num (int): 1\n",
    "    cell_indx (int): 0\n",
    "    cell (byte-string): ex b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00'\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    file_bytes = load_file(file_name)\n",
    "    page = load_page(file_bytes, page_num)\n",
    "    page = bytearray(page)\n",
    "\n",
    "    num_cells = struct.unpack(endian+'h', page[2:4])[0]\n",
    "    assert(cell_indx<=num_cells-1)#index starts at 0\n",
    "    assert(num_cells!=0) #delete CAN empty a page\n",
    "    assert(cell_indx>=0)\n",
    "\n",
    "    cell_content_area_start = struct.unpack(endian+'h', page[4:6])[0]\n",
    "    end_of_array = 16+2*num_cells\n",
    "    array_idx_top = 16+2*cell_indx\n",
    "    array_idx_bot = 16+2*(cell_indx+1)\n",
    "    available_bytes = page_available_bytes(file_bytes, page_num)\n",
    "    cell_top_idx, cell_bot_idx = get_cell_indices(page, cell_indx)\n",
    "    cell_2_update = page[cell_top_idx:cell_bot_idx]\n",
    "    if len(cell_2_update)==len(cell):\n",
    "        page[cell_top_idx:cell_bot_idx] = cell\n",
    "    elif len(cell_2_update)<len(cell): #need to shift cell_content up\n",
    "        dis2move =  len(cell) - len(cell_2_update)\n",
    "        assert(dis2move<=available_bytes)   #NEED TO SPLIT\n",
    "        page = shift_page_content(page, cell_content_area_start, cell_top_idx, dis2move, up=True)\n",
    "        #since we just shifted every cell, every value in cell_array is off\n",
    "        page = update_array_values(page, cell_indx, num_cells, dis2move, up=True)\n",
    "        #change cell content area start\n",
    "        page[4:6] = struct.pack(endian+'h', cell_content_area_start-dis2move)\n",
    "        #insert updated cell\n",
    "        page[cell_top_idx-dis2move:cell_bot_idx] = cell\n",
    "\n",
    "    else: #need to shift cell_content up\n",
    "        dis2move =  len(cell_2_update) - len(cell)\n",
    "        page = shift_page_content(page, cell_content_area_start, cell_top_idx, dis2move, up=False)\n",
    "        #since we just shifted every cell, every value in cell_array is off\n",
    "        page = update_array_values(page, cell_indx, num_cells, dis2move, up=True)\n",
    "        #change cell content area start\n",
    "        page[4:6] = struct.pack(endian+'h', cell_content_area_start+dis2move)\n",
    "        page[cell_top_idx+dis2move:cell_bot_idx] = cell\n",
    "\n",
    "    save_page(file_name, page_num, page)\n",
    "    assert(len(page)==PAGE_SIZE) #ensure page is same size\n",
    "    return None\n",
    "\n",
    "\n",
    "def update_page_header(file_name, page_num, rsibling_rchild=None, is_interior=None, parent=None):\n",
    "    \"\"\"Updates the page header. for use in inserts/deletes\"\"\"\n",
    "    is_table = file_name[-4:]=='.tbl'\n",
    "    is_index=not is_table\n",
    "    is_leaf = not is_interior\n",
    "    file_bytes = load_file(file_name)\n",
    "    page = load_page(file_bytes, page_num)\n",
    "    page = bytearray(page)\n",
    "    if rsibling_rchild is not None:\n",
    "        assert(len(file_bytes)/PAGE_SIZE>=rsibling_rchild)\n",
    "        page[6:10] = struct.pack(endian+'i', rsibling_rchild)\n",
    "    if is_interior is not None:\n",
    "        if page[0] in [5,13]:\n",
    "            is_table = True\n",
    "        else:\n",
    "            rchild_num = write_new_page(table_name, is_table, is_interior, rsibling_rchild, parent_num)\n",
    "            lchild_num = write_new_page(table_name, is_table, is_interior, rchild_num, parent_num)\n",
    "\n",
    "        for i in range(middle_cell):\n",
    "            #EXTRACT CELL VALUES (ROWID)\n",
    "            cell = table_create_cell(schema, value_list, is_interior, left_child_page=None,  rowid=None)\n",
    "            page_insert_cell(file_name, lchild_num, cell)\n",
    "\n",
    "        for i in range(middle_cell, num_cells):\n",
    "            #EXTRACT CELL VALUES (ROWID)\n",
    "            cell = table_create_cell(schema, value_list, is_interior, left_child_page=None,  rowid=None)\n",
    "            page_insert_cell(file_name, lchild_num, cell)\n",
    "    else:\n",
    "        try:\n",
    "            #insert(cell into paretn)\n",
    "            pass\n",
    "        except:\n",
    "            #bplus_split_page(parent)\n",
    "            pass\n",
    "\n",
    "        if is_table and is_interior:\n",
    "            page[0:1] = b'\\x05'\n",
    "        elif is_table and is_leaf:\n",
    "            page[0:1] = b'\\x0d'\n",
    "        elif is_index and is_interior:\n",
    "            page[0:1] = b'\\x02'\n",
    "        elif is_index and is_leaf:\n",
    "            page[0:1] = b'\\x0a'\n",
    "    if parent is not None:\n",
    "        page[10:14] = struct.pack(endian+'i', parent)\n",
    "    save_page(file_name, page_num, page)\n",
    "    return None\n",
    "\n",
    "\n",
    "def update_cell_lpointer(file_name, page_num, cell_indx, lpointer):\n",
    "    file_bytes = load_file(file_name)\n",
    "    page = load_page(file_bytes, page_num)\n",
    "    page = bytearray(page)\n",
    "    cell_top_idx, cell_bot_idx = get_cell_indices(page, cell_indx)\n",
    "    page[cell_top_idx:cell_top_idx+4] = struct.pack(endian+'i', lpointer)\n",
    "    save_page(file_name, page_num, page)\n",
    "    return None\n",
    "\n",
    "\n",
    "def load_file(file_name):\n",
    "    \"\"\"loads the table/index file returns the bytestring for the entire file (reduce number of read/writes)\n",
    "\n",
    "    Parameters:\n",
    "    file (byte-string): ex 'taco.tbl'\n",
    "    page_num (int): 1\n",
    "\n",
    "    Returns:\n",
    "    page (bytestring):\n",
    "    \"\"\"\n",
    "    with open(file_name, 'rb') as f:\n",
    "        return f.read()\n",
    "\n",
    "\n",
    "def load_page(file_bytes, page_num):\n",
    "    \"\"\"\n",
    "    loads the page of from the table/index PAGE NUMBER STARTS AT ZERO, will only load one pa\n",
    "    Parameters:\n",
    "    file_name (string): ex 'taco.tbl'\n",
    "    page_num (int): 1\n",
    "    Returns:\n",
    "    page (bytestring):\n",
    "    \"\"\"\n",
    "    file_offset = page_num*PAGE_SIZE\n",
    "    return file_bytes[file_offset:(page_num+1)*PAGE_SIZE]\n",
    "\n",
    "\n",
    "def read_cells_in_page(file_bytes, page_num):\n",
    "    \"\"\"read all the data from a page, get the file_bytes object with load_file(file_name)\"\"\"\n",
    "    assert(page_num<(len(file_bytes)/PAGE_SIZE))\n",
    "    page = load_page(file_bytes, page_num)\n",
    "    num_cells = struct.unpack(endian+'h', page[2:4])[0]\n",
    "    parent_page = struct.unpack(endian+'i', page[10:14])[0]\n",
    "    available_bytes = page_available_bytes(file_bytes, page_num)\n",
    "    if page[0] in [5,13]:\n",
    "        is_table = True\n",
    "    else:\n",
    "        is_table = False\n",
    "\n",
    "    if page[0] in [2,5]:\n",
    "        is_interior = True\n",
    "    else:\n",
    "        is_interior = False\n",
    "\n",
    "    i=0\n",
    "    data = []\n",
    "    while i<=num_cells-1:\n",
    "        if i == 0:\n",
    "            cell_bot_loc = PAGE_SIZE\n",
    "        else:\n",
    "            cell_bot_loc = struct.unpack(endian+'h',page[16+2*(i-1):16+2*(i)])[0]\n",
    "        cell_top_loc = struct.unpack(endian+'h',page[16+2*i:16+2*(i+1)])[0]\n",
    "        cell = page[cell_top_loc:cell_bot_loc]\n",
    "        if is_table:\n",
    "            data.append(table_read_cell(cell, is_interior))\n",
    "        else:\n",
    "            data.append(index_read_cell(cell, is_interior))\n",
    "        i+=1\n",
    "\n",
    "    result = {\n",
    "    \"page_number\":page_num,\n",
    "    \"parent_page\":parent_page,\n",
    "    \"is_table\": is_table,\n",
    "    \"is_leaf\": not is_interior,\n",
    "    \"num_cells\":num_cells,\n",
    "    \"available_bytes\":available_bytes\n",
    "    }\n",
    "    if is_interior:\n",
    "        result['rightmost_child_page'] = parent_page = struct.unpack(endian+'i', page[6:10])[0]\n",
    "    else:\n",
    "        result['right_sibling_page'] = parent_page = struct.unpack(endian+'i', page[6:10])[0]\n",
    "    result['cells']=data\n",
    "    return result\n",
    "\n",
    "\n",
    "def read_all_pages_in_file(file_name):\n",
    "    \"\"\"\n",
    "    Given the file name, loads all data from every page. This is what we will use during inserts updates, deletes\n",
    "\n",
    "    Parameters:\n",
    "    file_name (string): ex\"davisbase_tables.tbl\"\n",
    "\n",
    "    Returns:\n",
    "    pages (dict of dicts): ex. b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00'\n",
    "\n",
    "    \"\"\"\n",
    "    if file_name[-3:]=='tbl':\n",
    "        is_table=True\n",
    "    else:\n",
    "        is_table = False\n",
    "\n",
    "    file = load_file(file_name)\n",
    "    file_size = len(file)\n",
    "    assert(file_size%PAGE_SIZE==0)\n",
    "    num_pages = int(file_size/PAGE_SIZE)\n",
    "    data = []\n",
    "    for page_num in range(num_pages):\n",
    "        data.append(read_cells_in_page(file, page_num))\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_indexes(table_name):\n",
    "    \"\"\"Returns all filenames for indexes of the table\"\"\"\n",
    "    indexes=[]\n",
    "    for filename in os.listdir():\n",
    "        if (filename[:len(table_name)]==table_name) and (filename[-4:]=='.ndx'):\n",
    "            indexes.append(filename)\n",
    "    return indexes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_next_page_rowid(table_name):\n",
    "    \"\"\"Finds the next rowid and page for an insert\"\"\"\n",
    "    pages = read_all_pages_in_file(table_name+'.tbl')\n",
    "    final_page_num = 0\n",
    "    while not pages[final_page_num]['is_leaf']:\n",
    "        final_page_num = pages[final_page_num]['rightmost_child_page']\n",
    "\n",
    "    final_page = pages[final_page_num]\n",
    "    if len(pages[0]['cells'])==0:#if there are no records in the table\n",
    "        next_rowid=0\n",
    "    else:\n",
    "        rowid_sorted_cells = sorted(final_page['cells'], key=lambda x: x['rowid'])\n",
    "        next_rowid = rowid_sorted_cells[-1]['rowid']\n",
    "    return final_page['page_number'], next_rowid + 1\n",
    "\n",
    "\n",
    "def get_column_names_from_catalog(table_name):\n",
    "    \"\"\"Returns the column names for a table in order\"\"\"\n",
    "    schema, catalog_cells = schema_from_catalog(table_name, with_rowid=True)\n",
    "    col_names = []\n",
    "    for cell in catalog_cells:\n",
    "        col_names.append((cell['data'][3],cell['data'][1])) #list of [(ord_pos, col_name)]\n",
    "    col_names = sorted(col_names, key=lambda x: x[0])\n",
    "    return  [i[1] for i in col_names]\n",
    "\n",
    "\n",
    "def schema_from_catalog(table_name, with_rowid=False):\n",
    "    \"\"\"Returns the column datatypes and a list of cells from davisbase_tables\"\"\"\n",
    "    data = read_all_pages_in_file('davisbase_columns.tbl')\n",
    "    all_cells = []\n",
    "    all_data = []\n",
    "    for page in data:\n",
    "        if not page['is_leaf']:\n",
    "            continue\n",
    "        for cell in page['cells']:\n",
    "            col_table = cell['data'][0].lower()\n",
    "            if col_table==table_name.lower():\n",
    "                col_name = cell['data'][1].lower()\n",
    "                if col_name=='rowid' and not with_rowid:\n",
    "                    continue\n",
    "                all_cells.append((cell['data'][3],cell['data'][2])) #list of [(ord_pos, dtype)]\n",
    "                all_data.append(cell)\n",
    "    all_cells = sorted(all_cells, key=lambda x: x[0])\n",
    "    schema = [i[1] for i in all_cells]\n",
    "    return schema, all_data\n",
    "\n",
    "\n",
    "def index_insert_cell_in_page(file_name, page_num, cell, cell_indx):\n",
    "    \"\"\"Inserts a cell into the middle of a page (rather than end)\"\"\"\n",
    "    file_bytes = load_file(file_name)\n",
    "    page = load_page(file_bytes, page_num)\n",
    "    page = bytearray(page)\n",
    "\n",
    "    num_cells = struct.unpack(endian+'h', page[2:4])[0]\n",
    "    if cell_index == num_cells: #add to end of page\n",
    "        page_insert_ce(file_name, page_number, cell)\n",
    "        return None\n",
    "\n",
    "    assert(cell_indx<=num_cells-1)#index starts at 0\n",
    "    assert(cell_indx>=0)\n",
    "    assert(len(cell)<page_available_bytes(file_bytes, page_num))\n",
    "    cell_content_area_start = struct.unpack(endian+'h', page[4:6])[0]\n",
    "    end_of_array = 16+2*num_cells\n",
    "    array_idx_top = 16+2*cell_indx\n",
    "    array_idx_bot = 16+2*(cell_indx+1)\n",
    "\n",
    "    cell_top_loc, cell_bot_loc = get_cell_indices(page, cell_indx)\n",
    "    dis2move= len(cell)\n",
    "    #shift cell content down\n",
    "    page = shift_page_content(page, cell_content_area_start, cell_bot_loc, dis2move, up=True)\n",
    "    #since we just shifted every cell, every value in cell_array is off\n",
    "    page = update_array_values(page, cell_indx, num_cells, dis2move, up=True)\n",
    "    #change the cell_start area\n",
    "    page[4:6] = struct.pack(endian+'h', cell_content_area_start-dis2move)\n",
    "    #shift cell array up (deletes entry for deleted cell)\n",
    "    page = shift_page_content(page, array_idx_top, end_of_array, 2, up=False)\n",
    "    page[array_idx_top:array_idx_bot] = struct.pack(endian+'h', cell_bot_loc-dis2move)\n",
    "    page[cell_bot_loc-dis2move:cell_bot_loc] = cell\n",
    "    #update num of cells\n",
    "    page[2:4] = struct.pack(endian+'h', num_cells+1)\n",
    "    assert(len(page)==PAGE_SIZE)\n",
    "    save_page(file_name, page_num, page)\n",
    "    #ensure page is same size\n",
    "    return (num_cells - 1) == 0\n",
    "\n",
    "\n",
    "def print_it(file_name, page_format=False, limit=None):\n",
    "    \"\"\"Used for testing, prints all contents of a table/index in page form or as list\"\"\"\n",
    "    pages  =read_all_pages_in_file(file_name)\n",
    "    print(file_name[:-4])\n",
    "    if page_format:\n",
    "        for page in pages:\n",
    "            if page[\"is_leaf\"]:\n",
    "                continue\n",
    "            else:\n",
    "                print()\n",
    "                print(\"page_number: \",page['page_number'])\n",
    "                print(\"parent_page: \",page['parent_page'])\n",
    "                print(\"right_child_page: \",page['rightmost_child_page'])\n",
    "                print(\"bytes remaining:\", page['available_bytes'])\n",
    "                for cell in page[\"cells\"]:\n",
    "                    if file_name[-4:]=='.tbl':\n",
    "                        print(\"rowid: \",cell['rowid'],\"left child: \",cell['left_child_page'])\n",
    "                    else:\n",
    "                        print(\"indx_val: \",cell['index_value'],\"left child: \",cell['left_child_page'])\n",
    "        for page in pages:\n",
    "            if not page[\"is_leaf\"]:\n",
    "                continue\n",
    "            else:\n",
    "                print()\n",
    "                print(\"page_number: \",page['page_number'])\n",
    "                print(\"parent_page: \",page['parent_page'])\n",
    "                print(\"right_sibling_page: \",page['right_sibling_page'])\n",
    "                print(\"bytes remaining:\", page['available_bytes'])\n",
    "                rowids = []\n",
    "                for cell in page[\"cells\"]:\n",
    "                    if file_name[-4:]=='.tbl':\n",
    "                        rowids.append(cell['rowid'])\n",
    "                    else:\n",
    "                        rowids.append(cell['index_value'])\n",
    "                print(rowids)\n",
    "    else:\n",
    "        rows = []\n",
    "        for page in pages:\n",
    "            if not page[\"is_leaf\"]:\n",
    "                continue\n",
    "            else:\n",
    "                for cell in page[\"cells\"]:\n",
    "                    if file_name[-4:]=='.tbl':\n",
    "                        rows.append([cell['rowid']]+cell['data'])\n",
    "                    else:\n",
    "                        rows.append([cell['index_value'],cell['assoc_rowids']])\n",
    "        rows = sorted(rows, key=lambda x: x[0])\n",
    "        i=1\n",
    "        for row in rows:\n",
    "            if limit!=None and i>limit:\n",
    "                break\n",
    "            print(row)\n",
    "            i+=1\n",
    "\n",
    "\n",
    "def add_rowid_to_cell(file_name, page_num, cell_indx, rowid, cell):\n",
    "    \"\"\"Used in index insert, adds a rowid to list of associated rowids\n",
    "    This is done when an indec_value is already present in the index\"\"\"\n",
    "    cell_binary = cell['cell_binary']+struct.pack(endian+'i', rowid)\n",
    "    try:\n",
    "        page_update_cell(file_name, page_num, cell_indx, cell_binary)\n",
    "    except:\n",
    "        return\n",
    "\n",
    "\n",
    "def get_all_table_cells(table_name):\n",
    "    \"\"\"Grabs all the cells (no order)\"\"\"\n",
    "    pages  =read_all_pages_in_file(file_name)\n",
    "    cells = []\n",
    "    for page in pages:\n",
    "        if page[\"is_leaf\"]:\n",
    "            continue\n",
    "        else:\n",
    "            for cell in page[\"cells\"]:\n",
    "                cells.append(cell)\n",
    "    return cells\n",
    "\n",
    "###########################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###########################################################################\n",
    "# DONE, BUT NEEDS CONNECTING\n",
    "\"\"\"NEEDS CONNECTING TO CREATE_TABLE_PARSER\"\"\"\n",
    "def create_table(command):\n",
    "    \"\"\"Given the inputs of the command line, creates table, metadata, and indexes\"\"\"\n",
    "    col_catalog_dictionary = parse_create_table(command)\n",
    "    table_name = list(col_catalog_dictionary.keys())[0]\n",
    "    initialize_file(table_name, True)\n",
    "    catalog_add_table(col_catalog_dictionary)\n",
    "    initialize_indexes(col_catalog_dictionary)\n",
    "    return None\n",
    "\n",
    "\"\"\"NEEDS CONNECTING TO CREATE_INDEX_PARSER\"\"\"\n",
    "def create_index(command):\n",
    "    \"\"\"Given the inputs of the command line, creates index on an existing table\"\"\"\n",
    "    table_name, column_name = parse_create_index(command)\n",
    "    index_name = table_name+'_'+column_name\n",
    "    initialize_file(index_name, False) #create the index\n",
    "    columns = get_column_names_from_catalog(table_name)[1:] #column names minus rowid\n",
    "    schema, _ = schema_from_catalog(table_name)\n",
    "    ord_position =  columns.index(column_name) #position of data\n",
    "    index_dtype = schema[ord_position]\n",
    "    cells = get_all_table_cells(table_name)\n",
    "    for cell in cells:\n",
    "        rowid = cell['rowid']\n",
    "        index_value = cell['data'][ord_position]\n",
    "        index_insert(table_name, column_name, index_dtype, index_value, rowid)\n",
    "\n",
    "\"\"\"NEEDS CONNECTING TO CREATE_INDEX_PARSER\"\"\"\n",
    "\"\"\"Also need a function to check for uniqueness, not nullness\"\"\"\n",
    "def insert_into(command):\n",
    "    \"\"\"values would be a list of length self.columns, NULL represented as None\"\"\"\n",
    "    \"\"\"Parser needs to return \"table_name\", [[col1,col2,col3],[col1,col2,col3],[col1,col2,col3]]\"\"\"\n",
    "    table_name, values = parse_insert_into(command)\n",
    "    violation_flag, violating_row = FUNCTION_TO_CHECK_CONSTRAINTS_THAT_WE_DONT_HAVE_YET(table_name, values)\n",
    "    if violation_flag: #if violation fail insert\n",
    "        print(\"Constraint violated for row {}\".format(violating_row))\n",
    "        return None\n",
    "    schema, all_col_data = schema_from_catalog(table_name)\n",
    "    col_names = get_column_names_from_catalog(table_name)[1:]\n",
    "    indexes = get_indexes(table_name)\n",
    "    for val in values:\n",
    "        next_page, next_rowid = get_next_page_rowid(table_name)\n",
    "        cell = table_create_cell(schema, values, False,  rowid=next_rowid)\n",
    "        try:\n",
    "            page_insert_cell(table_name+'.tbl', next_page, cell)\n",
    "        except:\n",
    "            table_leaf_split_page(table_name+'.tbl', next_page, cell)\n",
    "        for filename in indexes:\n",
    "            index_colname = filename[len(table_name)+1:-4]\n",
    "            i = col_names.index(index_colname)\n",
    "            index_dtype= schema[i]\n",
    "            index_value= val[i] #index by ord position\n",
    "            index_insert(table_name, index_colname, index_dtype, index_value, next_rowid)\n",
    "\n",
    "\n",
    "def delete_from(command):\n",
    "    table_name, condition = parse_delete_from(command)\n",
    "    cells = WHERE_FUNCTION(table_name, condition)\n",
    "    col_names = get_column_names_from_catalog(table_name)[1:]\n",
    "    indexes = get_indexes(table_name)\n",
    "    for cell in cells:\n",
    "        table_delete(table_name, cell)\n",
    "        for filename in indexes:\n",
    "            index_colname = filename[len(table_name)+1:-4]\n",
    "            i = col_names.index(index_colname) #get position\n",
    "            index_value = cell['data'][i] #index by ord position\n",
    "            index_delete(table_name, index_colname, index_value, cell['rowid'])\n",
    "\n",
    "\n",
    "def update(command):\n",
    "    \"\"\"\n",
    "    dict_new_values = {\n",
    "    \"column1\":new_value_to_update_to,\n",
    "    \"column2\":new_value_to_update_to,\n",
    "    \"column4\":new_value_to_update_to,\n",
    "    }\"\"\"\n",
    "    table_name, condition, dict_new_values = parse_delete_from(command)\n",
    "    cells = WHERE_FUNCTION(table_name, condition)\n",
    "    col_names = get_column_names_from_catalog(table_name)[1:]\n",
    "    indexes = get_indexes(table_name)\n",
    "    for cell in cells:\n",
    "        table_update(table_name, cell, dict_new_values)\n",
    "        for filename in indexes:\n",
    "            index_colname = filename[len(table_name)+1:-4]\n",
    "            if index_colname in dict_new_values:\n",
    "                i = col_names.index(index_colname) #get position\n",
    "                index_value = cell['data'][i] #index by ord position\n",
    "                new_index_value = dict_new_values[index_colname]\n",
    "                index_update(table_name, index_colname, index_value, cell['rowid'], new_index_value)\n",
    "\n",
    "\n",
    "#########################################################################\n",
    "# TESTING\n",
    "\n",
    "def page_insert_cell(file_name, page_num, cell):\n",
    "    \"\"\"\n",
    "    Inserts a bytestring into a page from a table or index file. Updates the page header. Fails if page-full\n",
    "\n",
    "    Parameters:\n",
    "    file_name (string): ex 'taco.tbl'\n",
    "    page_num (int): 1\n",
    "    cell (byte-string): ex b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00'\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    file_bytes = load_file(file_name)\n",
    "    page = load_page(file_bytes, page_num)\n",
    "    page = bytearray(page)\n",
    "\n",
    "    if type(cell)==list:\n",
    "        cells = cell\n",
    "        for cell in cells:\n",
    "            assert(len(cell)<page_available_bytes(file_bytes, page_num)) #CHECK IF PAGE FULL\n",
    "            num_cells = struct.unpack(endian+'h', page[2:4])[0]\n",
    "            bytes_from_top = 16+(2*num_cells)\n",
    "            bytes_from_bot =struct.unpack(endian+'h', page[4:6])[0]\n",
    "            new_start_index = bytes_from_bot - len(cell)\n",
    "            #insert cell data\n",
    "            page[new_start_index:bytes_from_bot] = cell\n",
    "            #add to 2byte cell array\n",
    "            page[bytes_from_top:bytes_from_top+2] = struct.pack(endian+'h', new_start_index)\n",
    "            #update start of cell content\n",
    "            page[4:6] = struct.pack(endian+'h', new_start_index)\n",
    "            #update num_cells\n",
    "            page[2:4] = struct.pack(endian+'h', num_cells+1)\n",
    "            assert(len(page)==PAGE_SIZE)\n",
    "    else:\n",
    "        assert(len(cell)<page_available_bytes(file_bytes, page_num)) #CHECK IF PAGE FULL\n",
    "        num_cells = struct.unpack(endian+'h', page[2:4])[0]\n",
    "        bytes_from_top = 16+(2*num_cells)\n",
    "        bytes_from_bot =struct.unpack(endian+'h', page[4:6])[0]\n",
    "        new_start_index = bytes_from_bot - len(cell)\n",
    "        #insert cell data\n",
    "        page[new_start_index:bytes_from_bot] = cell\n",
    "        #add to 2byte cell array\n",
    "        page[bytes_from_top:bytes_from_top+2] = struct.pack(endian+'h', new_start_index)\n",
    "        #update start of cell content\n",
    "        page[4:6] = struct.pack(endian+'h', new_start_index)\n",
    "        #update num_cells\n",
    "        page[2:4] = struct.pack(endian+'h', num_cells+1)\n",
    "        assert(len(page)==PAGE_SIZE)\n",
    "    save_page(file_name, page_num, page)\n",
    "    return None\n",
    "\n",
    "\n",
    "def page_delete_cells_on_and_after(file_name, page_num, cell_indx):\n",
    "    \"\"\"Deletes all cells in page on or after cell_indx (starts w zero)\"\"\"\n",
    "    file_bytes = load_file(file_name)\n",
    "    page = load_page(file_bytes, page_num)\n",
    "    page = bytearray(page)\n",
    "    num_cells = struct.unpack(endian+'h', page[2:4])[0]\n",
    "    assert(cell_indx<=num_cells-1)#index starts at 0\n",
    "    assert(num_cells>=1) #delete CAN empty a page\n",
    "    assert(cell_indx>=0)\n",
    "    cell_content_area_start = struct.unpack(endian+'h', page[4:6])[0]\n",
    "    cell_top_loc, cell_bot_loc = get_cell_indices(page, cell_indx)\n",
    "    dis = cell_bot_loc - cell_content_area_start\n",
    "    page[cell_content_area_start:cell_bot_loc] = b'\\x00'*dis\n",
    "    page[16+2*cell_indx:16+2*num_cells] = b'\\x00'*(num_cells-cell_indx)\n",
    "    #update num of cells\n",
    "    page[2:4] = struct.pack(endian+'h', num_cells-(cell_indx+1))\n",
    "    save_page(file_name, page_num, page)\n",
    "    assert(len(page)==PAGE_SIZE) #ensure page is same size\n",
    "    return (num_cells - 1) == 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def table_interior_split_page(file_name, split_page_num, cell2insert, new_rightmost_page):\n",
    "    pages = read_all_pages_in_file(file_name)\n",
    "    values = pages[split_page_num]\n",
    "\n",
    "    table_name = file_name[:-4]\n",
    "    parent_num = values['parent_page']\n",
    "    is_interior = not values['is_leaf']\n",
    "    is_table = values['is_table']\n",
    "    assert(is_table)\n",
    "    assert(is_interior)\n",
    "\n",
    "    num_cells = values['num_cells']\n",
    "    cells = values['cells']\n",
    "    middle_cell = int((num_cells+1)//2) #have to add one since we havent actually added the cell\n",
    "\n",
    "\n",
    "    middle_cell_binary = cells[middle_cell]['cell_binary']\n",
    "    middle_rowid = cells[middle_cell]['rowid']\n",
    "\n",
    "    rightmost_child_page_right = new_rightmost_page\n",
    "    rightmost_child_page_left = cells[middle_cell]['left_child_page']\n",
    "\n",
    "\n",
    "    if parent_num==-1: #ROOT CONDITION #children will also be interior nodes\n",
    "        rchild_num = write_new_page(table_name, is_table, is_interior, new_rightmost_page, split_page_num)\n",
    "        lchild_num = write_new_page(table_name, is_table, is_interior, rightmost_child_page_left, split_page_num)\n",
    "\n",
    "        cells2copy=[]\n",
    "        for i in range(middle_cell):\n",
    "            cells2copy.append(cells[i]['cell_binary'])\n",
    "            update_page_header(file_name, cells[i]['left_child_page'], parent=lchild_num)\n",
    "        update_page_header(file_name, rightmost_child_page_left, parent=lchild_num) #update parent of rightmost\n",
    "        #Copy cells into left child\n",
    "        page_insert_cell(file_name, lchild_num, cells2copy)\n",
    "\n",
    "        cells2copy=[]\n",
    "        for i in range(middle_cell, num_cells):#update child to point header to rchild\n",
    "        cells2copy.append(cells[i]['cell_binary'])\n",
    "            update_page_header(file_name, cells[i]['left_child_page'], parent=rchild_num)\n",
    "        update_page_header(file_name, rightmost_child_page_right, parent=rchild_num)\n",
    "        #Copy cells into right child\n",
    "        page_insert_cell(file_name, rchild_num, cells2copy)\n",
    "\n",
    "        #Update the pointers in the new, root node, then delete all but middle cell\n",
    "        page_delete_cells_on_and_after(file_name, split_page_num, 0)\n",
    "        page_insert_cell(file_name, split_page_num, middle_cell_binary)\n",
    "        update_cell_lpointer(file_name, split_page_num, 0, lchild_num)\n",
    "        update_page_header(file_name, split_page_num, rsibling_rchild=rchild_num)\n",
    "        return rchild_num  #return so we can update headers of pages that couldnt fit in the old page\n",
    "\n",
    "    else:\n",
    "        rsibling = write_new_page(table_name, is_table, is_interior, rightmost_child_page_right, parent_num)\n",
    "\n",
    "        cells2copy=[]\n",
    "        for i in range(middle_cell, num_cells): #Copy cells into right child\n",
    "            cells2copy.append(cells[i]['cell_binary'])\n",
    "            update_page_header(file_name, insert_order[i]['left_child_page'], parent=rsibling)\n",
    "        update_page_header(file_name, rightmost_child_page_right, parent=rsibling)\n",
    "        page_insert_cell(file_name, rchild_num, cells2copy)\n",
    "\n",
    "        #middle_cell_binary = table_create_cell([], [], True, left_child_page=split_page_num,  rowid=middle_rowid)\n",
    "        page_delete_cells_on_and_after(file_name, split_page_num, middle_cell)\n",
    "        update_page_header(file_name, split_page_num, rsibling_rchild=rightmost_child_page_left)\n",
    "\n",
    "        if pages[parent_num]['rightmost_child_page']==split_page_num:\n",
    "            update_page_header(file_name, parent_num, rsibling_rchild=rsibling)\n",
    "        try:\n",
    "            page_insert_cell(file_name, parent_num, middle_cell_binary)\n",
    "        except:\n",
    "            new_parent = table_interior_split_page(file_name, parent_num, middle_cell_binary, rsibling)\n",
    "            update_page_header(file_name, rsibling, parent = new_parent)\n",
    "            update_page_header(file_name, split_page_num, parent = new_parent)\n",
    "        return rsibling\n",
    "\n",
    "#could put these two together, but I dont care\n",
    "def table_leaf_split_page(file_name, split_page_num, cell2insert):\n",
    "    file_bytes = load_file(file_name)\n",
    "    values = read_cells_in_page(file_bytes, split_page_num)\n",
    "\n",
    "    table_name = file_name[:-4]\n",
    "    parent_num = values['parent_page']\n",
    "    is_interior = not values['is_leaf']\n",
    "    is_leaf = values['is_leaf']\n",
    "    is_table = values['is_table']\n",
    "    assert(is_table)\n",
    "    assert(is_leaf)\n",
    "\n",
    "    num_cells = values['num_cells']\n",
    "    cells = values['cells']\n",
    "    middle_cell = int((num_cells+1)/2) #have to add one since we havent actually added the cell\n",
    "    middle_cell_binary = cells[middle_cell]['cell_binary']\n",
    "    middle_rowid = cells[middle_cell]['rowid']\n",
    "    right_sibling_page = values['right_sibling_page']\n",
    "\n",
    "    if parent_num==-1: #IS ROOT ->create two children\n",
    "        rchild_num = write_new_page(table_name, is_table, False, -1, split_page_num)\n",
    "        lchild_num = write_new_page(table_name, is_table, False, rchild_num, split_page_num)\n",
    "\n",
    "        cells2copy = []\n",
    "        for i in range(middle_cell):   #Copy cells into left child\n",
    "            cells2copy.append(cells[i]['cell_binary'])\n",
    "        page_insert_cell(file_name, lchild_num, cells2copy)\n",
    "\n",
    "        cells2copy = []\n",
    "        for i in range(middle_cell, num_cells): #Copy cells into right child\n",
    "            cells2copy.append(cells[i]['cell_binary'])\n",
    "        page_insert_cell(file_name, rchild_num, cells2copy)\n",
    "        page_insert_cell(file_name, rchild_num, cell2insert)\n",
    "\n",
    "        middle_cell_binary = table_create_cell([], [], True, left_child_page=lchild_num,  rowid=middle_rowid)\n",
    "        update_page_header(file_name, split_page_num, rsibling_rchild=rchild_num, is_interior=True)\n",
    "        page_delete_cells_on_and_after(file_name, split_page_num, 0)\n",
    "        page_insert_cell(file_name, split_page_num, middle_cell_binary)\n",
    "\n",
    "    else: #Non-root ->propagate upward\n",
    "        rsibling = write_new_page(table_name, is_table, is_interior, right_sibling_page, parent_num)\n",
    "        update_page_header(file_name, split_page_num, rsibling_rchild=rsibling)\n",
    "\n",
    "        cells2copy = []\n",
    "        for i in range(middle_cell, num_cells): #Copy cells into right child\n",
    "            cells2copy.append(cells[i]['cell_binary'])\n",
    "        page_insert_cell(file_name, rsibling, cells2copy)\n",
    "        page_insert_cell(file_name, rsibling, cell2insert)\n",
    "\n",
    "        page_delete_cells_on_and_after(file_name, split_page_num, middle_cell)\n",
    "\n",
    "        update_page_header(file_name, parent_num, rsibling_rchild=rsibling)\n",
    "        middle_cell_binary = table_create_cell([], [], True, left_child_page=split_page_num,  rowid=middle_rowid)\n",
    "        try:\n",
    "            page_insert_cell(file_name, parent_num, middle_cell_binary)\n",
    "        except:\n",
    "            new_parent = table_interior_split_page(file_name, parent_num, middle_cell_binary, rsibling)\n",
    "            update_page_header(file_name,rsibling, parent = new_parent)\n",
    "            update_page_header(file_name, split_page_num, parent = new_parent)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def page_cell_indx_given_key(pages, index_value):\n",
    "    \"\"\"Given rowid, returns the page and cell where the cell should go\"\"\"\n",
    "    page_num=0\n",
    "    if len(pages[page_num]['cells'])==0:\n",
    "        return page_num, None\n",
    "    return get_page_cell_indx(pages, index_value, page_num)\n",
    "\n",
    "\n",
    "\n",
    "def get_page_cell_indx(pages, value, page_num):\n",
    "    \"\"\"Given rowid, returns the page and cell where the cell should go\"\"\"\n",
    "    page = pages['page_num']\n",
    "    is_table= page['is_table']\n",
    "    is_leaf = page['is_leaf']\n",
    "    if not is_table:\n",
    "        for i, cell in enumerate(page['cells']):\n",
    "            if cell['index_value']==index_value:\n",
    "                return page_num, i\n",
    "            elif cell['index_value'] > index_value:\n",
    "                if not page['is_leaf']:\n",
    "                    return get_page_cell_indx(pages, value, cell['left_child_page'])\n",
    "                else:\n",
    "                    return page_num, i\n",
    "            else:\n",
    "                if page['is_leaf'] and i+1==len(page['cells']):\n",
    "                    return page_num, len(page['cells'])\n",
    "                if not page['is_leaf'] and i+1==len(page['cells']):\n",
    "                    return get_page_cell_indx(pages, value, page['rightmost_child_page'])\n",
    "                else:\n",
    "                    continue\n",
    "    else:\n",
    "        for cell_indx, cell in enumerate(page['cells']):\n",
    "            if (cell['rowid'] == value):\n",
    "                if is_leaf: #got a match\n",
    "                    return page_num, cell_indx\n",
    "                else:\n",
    "                    continue #next iteration will get it\n",
    "            elif (cell['rowid'] > value): #same\n",
    "                if not is_leaf:\n",
    "                    return get_page_cell_indx(pages, value, cell['left_child_page'])\n",
    "                else:\n",
    "                    return page_num, cell_indx\n",
    "\n",
    "            elif (cell['rowid']<value ):\n",
    "                if not is_leaf:\n",
    "                    return get_page_cell_indx(pages, value, page['rightmost_child_page'])\n",
    "                else:\n",
    "                    return page_num, len(page['cells'])\n",
    "            else:\n",
    "                assert(False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "#IN PROGRESS\n",
    "\n",
    "\n",
    "\n",
    "def index_insert(table_name, column_name, index_dtype, index_value, rowid):\n",
    "    \"\"\"rowid will not be present, but will key value be present?\n",
    "    if kv present -> append rowid to cell\n",
    "    if kv not present -> create new cell insert to page\n",
    "\n",
    "    if finds kv-> if cell has room -> insert\n",
    "                ->if no room -> create_cell -> insert in left child\"\"\"\n",
    "    file_name = table_name+'_'+column_name+'.ndx'\n",
    "    pages = read_all_pages_in_file(file_name)\n",
    "\n",
    "    page_num, cell_indx = page_cell_indx_given_key(pages, index_value)\n",
    "    page = pages[page_num]\n",
    "\n",
    "    if len(page['cells'])!=cell_indx:\n",
    "        cell = page['cells'][cell_indx]\n",
    "        if cell['index_value']==index_value: #add to the list\n",
    "            if rowid not in cell['assoc_rowids']:\n",
    "                add_rowid_to_cell(file_name, page_num, cell_indx, rowid, cell)\n",
    "                return\n",
    "            else: #rowid already exists\n",
    "                return\n",
    "    else:\n",
    "        cell = index_create_cell(index_dtype, index_value, [rowid], False, left_child_page=None)\n",
    "        #running low on space\n",
    "        if pages[page_num]['available_bytes']/PAGE_SIZE<0.5:\n",
    "            index_leaf_split_page(file_name, page_num, cell, index_dtype, cell_indx)\n",
    "            return\n",
    "        else:\n",
    "            try:\n",
    "                index_insert_cell_in_page(file_name, page_num, cell, cell_indx)\n",
    "            except:\n",
    "                index_leaf_split_page(file_name, page_num, cell, index_dtype, cell_indx)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def index_interior_split_page(file_name, split_page_num, cell2insert, new_rightmost_page, cell_index):\n",
    "    pages = read_all_pages_in_file(file_name)\n",
    "    values = pages[split_page_num]\n",
    "\n",
    "    table_name = file_name[:-4]\n",
    "    parent_num = values['parent_page']\n",
    "    is_interior = not values['is_leaf']\n",
    "    is_table = values['is_table']\n",
    "    assert(not is_table)\n",
    "    assert(is_interior)\n",
    "\n",
    "    num_cells = values['num_cells']\n",
    "    cells = values['cells']\n",
    "    middle_cell = int((num_cells+1)//2) #have to add one since we havent actually added the cell\n",
    "\n",
    "    cell2insert=index_read_cell(cell2insert, is_interior)\n",
    "    insert_order = []\n",
    "    i=0\n",
    "    while len(insert_order)!=len(cells)+1:\n",
    "        if cell_index==i:\n",
    "            insert_order.append(cell2insert['cell_binary'])\n",
    "        else:\n",
    "            insert_order.append(cells[i]['cell_binary'])\n",
    "            i+=1\n",
    "\n",
    "    middle_cell_binary = insert_order[middle_cell]['cell_binary']\n",
    "    middle_index = insert_order[middle_cell]['index_value']\n",
    "    rightmost_child_page_right = new_rightmost_page\n",
    "\n",
    "    rightmost_child_page_left = insert_order[middle_cell]['left_child_page']\n",
    "\n",
    "    if parent_num==-1: #ROOT CONDITION #children will also be interior nodes\n",
    "        rchild_num = write_new_page(table_name, is_table, is_interior, new_rightmost_page, split_page_num)\n",
    "        lchild_num = write_new_page(table_name, is_table, is_interior, rightmost_child_page_left, split_page_num)\n",
    "\n",
    "        for i in range(middle_cell): #Copy cells into left child\n",
    "            update_page_header(file_name, insert_order[i]['left_child_page'], parent=lchild_num) #update child to point header to lchild\n",
    "        update_page_header(file_name, rightmost_child_page_left, parent=lchild_num)#update parent of rightmost\n",
    "        page_insert_cell(file_name, lchild_num, insert_order[:middle_cell])\n",
    "\n",
    "        for i in range(middle_cell, num_cells): #Copy cells into right child   #splitting the interior nodes,  the middle cell is not redundant in children\n",
    "            update_page_header(file_name, insert_order[i]['left_child_page'], parent=rchild_num)\n",
    "        update_page_header(file_name, rightmost_child_page_right, parent=rchild_num)\n",
    "        page_insert_cell(file_name, rchild_num, insert_order[middle_cell:])\n",
    "\n",
    "        page_delete_cell(file_name, rchild_num, 0)\n",
    "\n",
    "        page_delete_cells_on_and_after(file_name, split_page_num, 0)\n",
    "        page_insert_cell(file_name, split_page_num, middle_cell_binary)\n",
    "        update_page_header(file_name, split_page_num, rsibling_rchild=rchild_num)\n",
    "        update_cell_lpointer(file_name, split_page_num, 0, lchild_num)\n",
    "        return rchild_num  #return so we can update headers of pages that couldnt fit in the old page\n",
    "\n",
    "    else:\n",
    "        rsibling = write_new_page(table_name, is_table, is_interior, new_rightmost_page, parent_num)\n",
    "        update_page_header(file_name, split_page_num, rsibling_rchild=rightmost_child_page_left)\n",
    "\n",
    "        #delete all the copied cells from left sibling\n",
    "        page_delete_cells_on_and_after(file_name, split_page_num, 0)\n",
    "        for i in range(middle_cell): #Copy cells into right sibling\n",
    "            update_page_header(file_name, insert_order[i]['left_child_page'], parent=rsibling)\n",
    "        page_insert_cell(file_name, rsibling, insert_order[middle_cell:])\n",
    "\n",
    "        for i in range(middle_cell, num_cells): #Copy cells into right sibling\n",
    "            update_page_header(file_name, insert_order[i]['left_child_page'], parent=rsibling)\n",
    "        page_insert_cell(file_name, rsibling, insert_order[middle_cell:])\n",
    "        page_delete_cell(file_name, rsibling, 0)\n",
    "\n",
    "        if pages[parent_num]['rightmost_child_page']==split_page_num:\n",
    "            update_page_header(file_name, parent_num, rsibling_rchild=rsibling)\n",
    "\n",
    "        parent_page = read_cells_in_page(file_bytes, parent_num)\n",
    "        parent_cells = parent_page['cells']\n",
    "        for i, cell in enumerate(parent_cells):\n",
    "            if cell['index_value'] >  middle_index:\n",
    "                parent_index = i\n",
    "                update_cell_lpointer(file_name, parent_num, i, rsibling)\n",
    "                break\n",
    "            elif i==len(parent_cells)-1:\n",
    "                parent_index = len(parent_cells)\n",
    "\n",
    "\n",
    "        if parent_page['available_bytes']/PAGE_SIZE<0.5:\n",
    "            new_parent = index_interior_split_page(file_name, parent_num, middle_cell_binary, rsibling, parent_index)\n",
    "            update_page_header(file_name, rsibling, parent = new_parent)\n",
    "            update_page_header(file_name, split_page_num, parent = new_parent)\n",
    "        else:\n",
    "            try:\n",
    "                index_insert_cell_in_page(file_name, parent_num, middle_cell_binary, parent_index)\n",
    "            except:\n",
    "                new_parent = index_interior_split_page(file_name, parent_num, middle_cell_binary, rsibling, parent_index)\n",
    "                update_page_header(file_name, rsibling, parent = new_parent)\n",
    "                update_page_header(file_name, split_page_num, parent = new_parent)\n",
    "        return rsibling\n",
    "\n",
    "\n",
    "\n",
    "#could put these two together, but I dont care\n",
    "def index_leaf_split_page(file_name, split_page_num, cell2insert, index_dtype, index2insert):\n",
    "    file_bytes = load_file(file_name)\n",
    "    values = read_cells_in_page(file_bytes, split_page_num)\n",
    "\n",
    "    table_name = file_name[:-4]\n",
    "    parent_num = values['parent_page']\n",
    "    is_interior = not values['is_leaf']\n",
    "    is_table = values['is_table']\n",
    "    assert(not is_table)\n",
    "\n",
    "    num_cells = values['num_cells']\n",
    "    cells = values['cells']\n",
    "    middle_cell = int((num_cells+1)/2) #have to add one since we havent actually added the cell\n",
    "    cell2insert=index_read_cell(cell2insert, is_interior)\n",
    "    insert_order = []\n",
    "    i=0\n",
    "    while len(insert_order)!=len(cells)+1:\n",
    "        if cell_index==i:\n",
    "            insert_order.append(cell2insert['cell_binary'])\n",
    "        else:\n",
    "            insert_order.append(cells[i]['cell_binary'])\n",
    "            i+=1\n",
    "\n",
    "    middle_cell_binary = insert_order[middle_cell]\n",
    "    middle_index = insert_order[middle_cell]['index_value']\n",
    "\n",
    "    if parent_num==-1: #IS ROOT ->create two children\n",
    "        rchild_num = write_new_page(table_name, is_table, False, -1, split_page_num)\n",
    "        lchild_num = write_new_page(table_name, is_table, False, rchild_num, split_page_num)\n",
    "        page_insert_cell(file_name, lchild_num, insert_order[:middle_cell])\n",
    "        page_insert_cell(file_name, rchild_num, insert_order[middle_cell+1:])\n",
    "        page_delete_cells_on_and_after(file_name, split_page_num, 0)\n",
    "        middle_cell_binary = struct.pack(endian+'i', lchild_num) + middle_cell_binary\n",
    "        update_page_header(file_name, split_page_num, rsibling_rchild=rchild_num, is_interior=True)\n",
    "        page_insert_cell(file_name, split_page_num, middle_cell_binary)\n",
    "\n",
    "\n",
    "    else: #Non-root ->propagate upward\n",
    "        right_sibling_page = values['right_sibling_page']\n",
    "        rsibling = write_new_page(table_name, is_table, is_interior, right_sibling_page, parent_num)\n",
    "\n",
    "        page_delete_cells_on_and_after(file_name, split_page_num, 0)\n",
    "        page_insert_cell(file_name, lchild_num, insert_order[:middle_cell])\n",
    "        update_page_header(file_name, split_page_num, rsibling_rchild=rsibling)\n",
    "        page_insert_cell(file_name, rchild_num, insert_order[middle_cell+1:])\n",
    "        middle_cell_binary = struct.pack(endian+'i', split_page_num) + middle_cell_binary\n",
    "\n",
    "        parent_page = read_cells_in_page(file_bytes, parent_num)\n",
    "        parent_cells = parent_page['cells']\n",
    "\n",
    "        if parent_page[\"rightmost_child_page\"]==split_page_num:\n",
    "            update_page_header(file_name, parent_num, rsibling_rchild=rsibling)\n",
    "\n",
    "        for i, cell in enumerate(parent_cells):\n",
    "            if cell['index_value'] >  middle_index:\n",
    "                parent_index = i\n",
    "                update_cell_lpointer(file_name, parent_num, i, rsibling)\n",
    "                break\n",
    "            elif i==len(parent_cells)-1:\n",
    "                parent_index = len(parent_cells)\n",
    "\n",
    "        if parent_page['available_bytes']/PAGE_SIZE<0.5:\n",
    "            new_parent = index_interior_split_page(file_name, parent_num, middle_cell_binary, rsibling, middle=True, index2insert=parent_index)\n",
    "            update_page_header(file_name, rsibling, parent = new_parent)\n",
    "            update_page_header(file_name, split_page_num, parent = new_parent)\n",
    "            return None\n",
    "        else:\n",
    "            try:\n",
    "                index_insert_cell_in_page(file_name, parent_num, middle_cell_binary, parent_index)\n",
    "            except:\n",
    "                new_parent = index_interior_split_page(file_name, parent_num, middle_cell_binary, rsibling, middle=True, index2insert=parent_index)\n",
    "                update_page_header(file_name, rsibling, parent = new_parent)\n",
    "                update_page_header(file_name, split_page_num, parent = new_parent)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def delete(table_name, rowid):\n",
    "    page_num, cell_indx = page_cell_indx_given_index_value(table_name, rowid)\n",
    "    if cell_indx is None: #no value found\n",
    "        return None\n",
    "    else:\n",
    "        try:\n",
    "            page_delete_cell(file_name, page_num, cell_indx)\n",
    "        except:\n",
    "            table_leaf_merge_page(table_name+'.tbl', next_page, cell)\n",
    "\n",
    "        for filename in get_indexes(table_name):\n",
    "            for col in all_col_data:\n",
    "                if col['data'][1]==file_name[len(table_name)+1:-4]:\n",
    "                    index_dtype= col['data'][2]\n",
    "                    index_value= values[col['data'][3]] #index by ord position\n",
    "            next_page  = index_get_next_page(index_value)\n",
    "            try:\n",
    "                index_page_delete_cell(index_dtype, index_value, next_rowid)\n",
    "            except:\n",
    "                index_leaf_merge_page(table_name+'.tbl', next_page, cell)\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def update(table_name, new_values):\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "#TO DO\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def table_merge_pages(file_names, parent_num):\n",
    "    return None\n",
    "\n",
    "def index_merge_pages(file_names, parent_num):\n",
    "    return None\n",
    "\n",
    "def table_merge_pages(file_bytes, parent_num, lchild_num, rchild_num):\n",
    "    return None\n",
    "\n",
    "\n",
    "def catalog_drop_table(table_name):\n",
    "    \"\"\"\n",
    "    Deletes table_name.tbl, check if index exists (if so, delete index), update metadata remove all cells related to table_name\n",
    "\n",
    "    Returns:\n",
    "    bool: success_flag\n",
    "    \"\"\"\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "def drop_table(command):\n",
    "    table_name = parse_drop_table(command)\n",
    "    if check_table_exists(table_name):\n",
    "        success = delete_all_table_data(table_name)\n",
    "        if not success:\n",
    "            print(\"temporary error\")\n",
    "    else:\n",
    "        print(\"Table \\\"{}\\\" does note exist.\".format(table_name))\n",
    "\n",
    "\n",
    "def show_tables():\n",
    "    \"\"\"\n",
    "    This can be implemented by querying dabisbase_tables\n",
    "    \"\"\"\n",
    "    print(\"ALL TABLES\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def check_values_match_schema(values,schema):\n",
    "    \"\"\"Save coding time, assume will be correct\"\"\"\n",
    "    success = True\n",
    "    return True\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "\n",
    "\n",
    "#########################################################################\n",
    "#CLI FUNCTIONS\n",
    "\n",
    "\n",
    "\n",
    "#########################################################################\n",
    "# DDL FUNCTION\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_definitions(token_list):\n",
    "    '''\n",
    "    Subordinate function for create table to get column names and their definitions\n",
    "    '''\n",
    "    # assumes that token_list is a parenthesis\n",
    "    definitions = []\n",
    "    tmp = []\n",
    "    # grab the first token, ignoring whitespace. idx=1 to skip open (\n",
    "    tidx, token = token_list.token_next(1)\n",
    "    while token and not token.match(sqlparse.tokens.Punctuation, ')'):\n",
    "        tmp.append(token)\n",
    "        # grab the next token, this times including whitespace\n",
    "        tidx, token = token_list.token_next(tidx, skip_ws=False)\n",
    "        # split on \",\", except when on end of statement\n",
    "        if token and token.match(sqlparse.tokens.Punctuation, ','):\n",
    "            definitions.append(tmp)\n",
    "            tmp = []\n",
    "            tidx, token = token_list.token_next(tidx)\n",
    "    if tmp and isinstance(tmp[0], sqlparse.sql.Identifier):\n",
    "        definitions.append(tmp)\n",
    "    return definitions\n",
    "\n",
    "\n",
    "def parse_create_table(SQL):\n",
    "    \"\"\"\n",
    "    Parses the raw, lower-cased input from the CLI controller. Will identify table name,\n",
    "    column names, data types, and constraints. Will also check for syntax errors.\n",
    "    Also check that table_name is all characters (no punctuation spaces...)\n",
    "\n",
    "    Parameters:\n",
    "    command (string):  lower-case string from CLI.\n",
    "    (ex. \"CREATE TABLE table_name (\n",
    "             column_name1 data_type1 [NOT NULL][UNIQUE],\n",
    "             column_name2 data_type2 [NOT NULL][UNIQUE],\n",
    "            );\"\"  )\n",
    "\n",
    "    Returns:\n",
    "    tuple: (table_name, column_list, definition_list)\n",
    "\n",
    "    table_name: str\n",
    "    column_list: list of column objects.\n",
    "\n",
    "    SQL = \\\"\"\"CREATE TABLE foo (\n",
    "         id integer primary key,\n",
    "         title varchar(200) not null,\n",
    "         description text);\\\"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    if re.match(\"(?i)create (?i)table [a-zA-Z]+\\s\\(\\s?\\n?\", SQL):\n",
    "        if SQL.endswith(');'):\n",
    "            print(\"Valid statement\")\n",
    "    else:\n",
    "        print(\"Invalid statement\")\n",
    "\n",
    "    parsed = sqlparse.parse(SQL)[0]\n",
    "    table_name = str(parsed[4])\n",
    "    _, par = parsed.token_next_by(i=sqlparse.sql.Parenthesis)\n",
    "    columns = extract_definitions(par)\n",
    "    col_list = []\n",
    "    definition_list = []\n",
    "    for column in columns:\n",
    "        definitions = ''.join(str(t) for t in column).split(',')\n",
    "        for definition in definitions:\n",
    "            d = ' '.join(str(t) for t in definition.split())\n",
    "            col_list.append(definition.split()[0])\n",
    "            definition_list.append(d)\n",
    "\n",
    "    d = {}\n",
    "    d[table_name] = {}\n",
    "    for col, definition in zip(col_list, definition_list):\n",
    "        d[table_name][col] = definition\n",
    "\n",
    "    return d\n",
    "\n",
    "\n",
    "def parse_drop_table(command):\n",
    "    \"\"\"\n",
    "    Parses the raw, lower-cased input from the CLI controller. Will identify table name,\n",
    "    Will also check for syntax errors. Throw error if\n",
    "\n",
    "    Parameters:\n",
    "    command (string):  lower-case string from CLI. (ex. \"drop table table_name;\"\" )\n",
    "\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    ## check if the drop statement is correct or not\n",
    "    ## statement must compulsarily end with semicolon\n",
    "    query_match = \"(?i)DROP\\s+(.*?)\\s*(?i)TABLE\\s+[a-zA-Z]+\\;\"\n",
    "    if re.match(query_match, command):\n",
    "        stmt = sqlparse.parse(command)[0]\n",
    "        tablename = str(stmt.tokens[-2])\n",
    "    else:\n",
    "        print(\"Enter correct query\")\n",
    "    return tablename\n",
    "\n",
    "\n",
    "def delete_all_table_data(table_name):\n",
    "    \"\"\"\n",
    "    Deletes table_name.tbl, check if index exists (if so, delete index), update metadata remove all cells related to table_name\n",
    "\n",
    "    Returns:\n",
    "    bool: success_flag\n",
    "    \"\"\"\n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "def create_index(command):\n",
    "    print(\"create index \\'{}\\'\".format(command))\n",
    "    return None\n",
    "\n",
    "\n",
    "############################################################################\n",
    "#DML FUNCTIONS\n",
    "\n",
    "def insert_into(command):\n",
    "    '''\n",
    "    Assuming values are being set along the correct order of columns\n",
    "    '''\n",
    "    print(\"Insert into \\'{}\\'\".format(command))\n",
    "    query_match = \"insert into\\s+(.*?)\\s*((?i)values\\s(.*?)\\s*)?;\"\n",
    "    if re.match(query_match, command):\n",
    "        stmt = sqlparse.parse(command)[0]\n",
    "        table_name = str(stmt.tokens[4])\n",
    "        values = str(stmt.tokens[-2])\n",
    "        values = re.sub(\"\\s\", \"\", re.split(';',re.sub(\"(?i)values\",\"\",values))[0])\n",
    "        print(values,\"\\t\",table_name)\n",
    "    else:\n",
    "        print(\"Enter correct query\")\n",
    "\n",
    "def delete_from(command):\n",
    "    print(\"delete from \\'{}\\'\".format(command))\n",
    "    ## check if the update statement is correct or not\n",
    "    query_match = \"delete\\s+(.*?)\\s*(?i)from\\s+(.*?)\\s*((?i)where\\s(.*?)\\s*)?;\"\n",
    "    if re.match(query_match, command):\n",
    "        stmt = sqlparse.parse(command)[0]\n",
    "        where_clause = str(stmt.tokens[-1])\n",
    "        where_clause = re.sub(\"\\s\", \"\", re.split(';',re.sub(\"(?i)where\",\"\",where_clause))[0])\n",
    "        where_clause = re.split('=|>|<|>=|<=|\\s',where_clause)\n",
    "        tablename = str(stmt.tokens[-3]).split(\",\")\n",
    "        print(where_clause,\"\\t\",tablename)\n",
    "    else:\n",
    "        print(\"Enter correct query\")\n",
    "\n",
    "\n",
    "def update(command):\n",
    "    print(\"update \\'{}\\'\".format(command))\n",
    "    ## check if the update statement is correct or not\n",
    "    query_match = \"(?i)update\\s+(.*?)\\s*(?i)set\\s+(.*?)\\s*((?i)where\\s(.*?)\\s*)?;\"\n",
    "    if re.match(query_match, command):\n",
    "        stmt = sqlparse.parse(command)[0]\n",
    "        where_clause = str(stmt.tokens[-1])\n",
    "        where_clause = re.sub(\"\\s\", \"\", re.split(';',re.sub(\"(?i)where\",\"\",where_clause))[0])\n",
    "        where_clause = re.split('=|>|<|>=|<=|\\s',where_clause)\n",
    "        set_col = itemgetter(*[0,-1])(re.split('=|\\s',str(stmt.tokens[-3])))\n",
    "        tablename = str(stmt.tokens[2])\n",
    "        print(where_clause,\"\\t\", tablename,\"\\t\",set_col)\n",
    "        ## perform select logic\n",
    "    else:\n",
    "        print(\"Enter correct query\")\n",
    "\n",
    "##########################################################################\n",
    "#DQL FUNCTIONS\n",
    "\n",
    "def query(command: str):\n",
    "    '''\n",
    "    command : Select statement eg. select a.a,b.b,c from a,b where a.a = b.a;\n",
    "    return : None\n",
    "    '''\n",
    "    print(\"User wants to query {}\".format(command))\n",
    "    ## check if the select statement is correct or not\n",
    "    query_match = \"select\\s+(.*?)\\s*(?i)from\\s+(.*?)\\s*((?i)where\\s(.*?)\\s*)?;\"\n",
    "    if re.match(query_match, command):\n",
    "        stmt = sqlparse.parse(command)[0]\n",
    "        where_clause = str(stmt.tokens[-1])\n",
    "        where_clause = re.sub(\"\\s\", \"\", re.split(';',re.sub(\"(?i)where\",\"\",where_clause))[0])\n",
    "        where_clause = re.split('=|>|<|>=|<=|\\s',where_clause)\n",
    "        tablename = str(stmt.tokens[-3]).split(\",\")[0]\n",
    "        columns = str(stmt.tokens[2]).split(\",\")\n",
    "#         print(where_clause,\"\\t\",tablename,\"\\t\",columns)\n",
    "        return where_clause, tablename, columns\n",
    "    else:\n",
    "\n",
    "        return -1,-1,-1\n",
    "\n",
    "\n",
    "def select_from(SQL):\n",
    "\n",
    "    where_condition, table_name, columns =  query(SQL)\n",
    "    print(table_name, where_condition[0], where_condition[1])\n",
    "\n",
    "#     column_list = get_column_names_from_catalog(table_name)\n",
    "\n",
    "#     index = column_list.index(where_condition[0])\n",
    "\n",
    "    if where_condition == -1:\n",
    "        print(\"Enter correct query\")\n",
    "\n",
    "    flag = False\n",
    "    for node in read_all_pages_in_file(\"davisbase_columns.tbl\"):\n",
    "        if node['is_leaf'] :\n",
    "            for cell in node['cells']:\n",
    "                data = cell['data']\n",
    "\n",
    "                if data[0] == table_name and data[1] == where_condition[0]:\n",
    "                    if data[2] == 'INT' and data[3] == int(where_condition[1]):\n",
    "                        print(cell)\n",
    "                        flag = True\n",
    "                        break\n",
    "                    elif data[2] == 'TEXT' and data[3] == str(where_condition[1]):\n",
    "                        print(cell)\n",
    "                        flag = True\n",
    "                        break\n",
    "        if flag:\n",
    "            break\n",
    "\n",
    "#############################################################################\n",
    "PAGE_SIZE = 512\n",
    "BYTE_ORDER = sys.byteorder\n",
    "if BYTE_ORDER=='big':\n",
    "    endian = '>'\n",
    "elif BYTE_ORDER=='little':\n",
    "    endian = '<'\n",
    "\n",
    "if __name__== \"__main__\":\n",
    "    init()\n",
    "    print(\"DavisBase version 0.00.1 2019-11-21\")\n",
    "    print(\"Enter \\\"help;\\\" for usage hints.\")\n",
    "    exit_command = False\n",
    "    while not exit_command:\n",
    "        command = input(\"davisbase> \").lower()\n",
    "        exit_command = check_input(command)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from davisbase import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 4,

   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-12-03T21:10:49.198Z"
    }
   },
   "outputs": [],
   "source": [
    "from random import randint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T22:14:39.764851Z",
     "start_time": "2019-12-01T22:14:39.760879Z"
    }
   },
   "outputs": [],
   "source": [
    "os.remove('davisbase_columns.tbl')\n",
    "os.remove('davisbase_tables.tbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T22:14:39.979725Z",
     "start_time": "2019-12-01T22:14:39.952790Z"
    }
   },
   "outputs": [],
   "source": [
    "init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T22:14:41.028101Z",
     "start_time": "2019-12-01T22:14:41.008124Z"
    }
   },
   "outputs": [],
   "source": [
    "col_catalog_dictionary = {\n",
    "    'table_name':{\n",
    "        \"column1\":{\n",
    "            'data_type':\"int\",\n",
    "            'ordinal_position':1,\n",
    "            'is_nullable':'YES',\n",
    "            'unique':'NO',\n",
    "            'primary_key':'YES'\n",
    "            },\n",
    "        \"column2\":{\n",
    "            'data_type':\"int\",\n",
    "            'ordinal_position':2,\n",
    "            'is_nullable':'YES',\n",
    "            'unique':'NO',\n",
    "            'primary_key':'NO'\n",
    "            },\n",
    "        \"column3\":{\n",
    "            'data_type':\"int\",\n",
    "            'ordinal_position':3,\n",
    "            'is_nullable':'YES',\n",
    "            'unique':'NO',\n",
    "            'primary_key':'NO'\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "table_name = list(col_catalog_dictionary.keys())[0]\n",
    "initialize_file(table_name, True)\n",
    "catalog_add_table(col_catalog_dictionary)\n",
    "initialize_indexes(col_catalog_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T22:14:43.068810Z",
     "start_time": "2019-12-01T22:14:43.062826Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print_it(\"davisbase_columns.tbl\", page_format=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T20:50:37.077572Z",
     "start_time": "2019-12-01T20:50:37.072587Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "table_name\n",
      "\n",
      "page_number:  0\n",
      "parent_page:  -1\n",
      "right_sibling_page:  0\n",
      "bytes remaining: 496\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print_it(\"table_name.tbl\", page_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T20:34:56.665742Z",
     "start_time": "2019-12-01T20:34:56.660756Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "table_name_column1\n",
      "\n",
      "page_number:  0\n",
      "parent_page:  -1\n",
      "right_sibling_page:  0\n",
      "bytes remaining: 496\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print_it(\"table_name_column1.ndx\", page_format=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column Names from Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T20:34:57.383426Z",
     "start_time": "2019-12-01T20:34:57.377442Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rowid',\n",
       " 'table_name',\n",
       " 'column_name',\n",
       " 'data_type',\n",
       " 'ordinal_position',\n",
       " 'is_nullable',\n",
       " 'unique',\n",
       " 'primary_key']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_column_names_from_catalog(\"davisbase_columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schema From Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T20:34:58.147311Z",
     "start_time": "2019-12-01T20:34:58.141326Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'table_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-e58f71487c04>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mschema_from_catalog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtable_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'table_name' is not defined"
     ]
    }
   ],
   "source": [
    "schema_from_catalog(table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table Insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T20:34:59.631594Z",
     "start_time": "2019-12-01T20:34:59.626610Z"
    }
   },
   "outputs": [],
   "source": [
    "def insert(table_name, values):\n",
    "    \"\"\"values would be a list of length self.columns, NULL represented as None\"\"\"\n",
    "    schema, all_col_data = schema_from_catalog(table_name)\n",
    "    next_page, next_rowid = get_next_page_rowid(table_name)\n",
    "    cell = table_create_cell(schema, values, False,  rowid=next_rowid)\n",
    "    try:\n",
    "        page_insert_cell(table_name+'.tbl', next_page, cell)\n",
    "    except:\n",
    "        table_leaf_split_page(table_name+'.tbl', next_page, cell)\n",
    "\n",
    "    \"\"\"for filename in get_indexes(table_name):\n",
    "        indexed_colname = filename[len(table_name)+1:-4]\n",
    "        for col in all_col_data:\n",
    "            if col['data'][1]==indexed_colname:\n",
    "                index_dtype= col['data'][2]\n",
    "                index_value= values[col['data'][3]] #index by ord position\n",
    "        index_insert(table_name, indexed_colname, index_dtype, index_value, next_rowid)\"\"\"\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T20:35:25.557870Z",
     "start_time": "2019-12-01T20:35:01.456591Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "table_name\n",
      "[1, 98, 86, 97]\n",
      "[2, 92, 59, 76]\n",
      "[3, 16, 51, 37]\n",
      "[4, 21, 38, 88]\n",
      "[5, 63, 19, 28]\n",
      "[6, 28, 2, 87]\n",
      "[7, 29, 80, 30]\n",
      "[8, 42, 8, 80]\n",
      "[9, 7, 93, 5]\n",
      "[10, 33, 75, 16]\n",
      "[11, 36, 11, 49]\n",
      "[12, 11, 43, 43]\n",
      "[13, 58, 89, 45]\n",
      "[14, 25, 90, 73]\n",
      "[15, 56, 72, 6]\n",
      "[16, 83, 31, 45]\n",
      "[17, 94, 9, 60]\n",
      "[18, 33, 43, 38]\n",
      "[19, 57, 63, 82]\n",
      "[20, 79, 22, 39]\n",
      "[21, 91, 39, 97]\n",
      "[22, 15, 25, 11]\n",
      "[23, 63, 98, 39]\n",
      "[24, 10, 6, 100]\n",
      "[25, 29, 13, 22]\n"
     ]
    }
   ],
   "source": [
    "data = [[randint(0,100),randint(0,100),randint(0,100)]for i in range(2000)]\n",
    "\n",
    "for row in data:\n",
    "    insert('table_name', row)\n",
    "    \n",
    "print_it(\"table_name.tbl\", page_format=False, limit=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T20:35:26.038662Z",
     "start_time": "2019-12-01T20:35:25.578814Z"
    }
   },
   "outputs": [],
   "source": [
    "print_it(\"table_name.tbl\", page_format=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index inserts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T22:27:30.625583Z",
     "start_time": "2019-12-01T22:27:30.583723Z"
    }
   },
   "outputs": [],
   "source": [
    "def index_insert(table_name, column_name, index_dtype, index_value, rowid):\n",
    "    \"\"\"rowid will not be present, but will key value be present?\n",
    "    if kv present -> append rowid to cell\n",
    "    if kv not present -> create new cell insert to page\n",
    "\n",
    "    if finds kv-> if cell has room -> insert\n",
    "                ->if no room -> create_cell -> insert in left child\"\"\"\n",
    "    file_name = table_name+'_'+column_name+'.ndx'\n",
    "    pages = read_all_pages_in_file(file_name)\n",
    "\n",
    "    page_num = 0\n",
    "    stop=False\n",
    "    while not stop:\n",
    "        page = pages[page_num]\n",
    "        if len(page['cells'])==0:\n",
    "            res = (page_num, None)\n",
    "            break\n",
    "        for i, cell in enumerate(page['cells']):\n",
    "            if cell['index_value']==index_value:\n",
    "                if rowid not in cell['assoc_rowids']:\n",
    "                    add_rowid_to_cell(file_name, page_num, i, rowid, cell)\n",
    "                res = (None, None)\n",
    "                stop = True\n",
    "                break\n",
    "            elif cell['index_value'] > index_value:\n",
    "                if not page['is_leaf']:\n",
    "                    page_num = cell['left_child_page']\n",
    "                    break\n",
    "                else:\n",
    "                    res = (page_num, i)\n",
    "                    stop = True\n",
    "                    break\n",
    "            else:\n",
    "                if page['is_leaf'] and i+1==len(page['cells']):\n",
    "                    res = (page_num, None)\n",
    "                    stop = True\n",
    "                    break\n",
    "                if not page['is_leaf'] and i+1==len(page['cells']):\n",
    "                    page_num = page['rightmost_child_page']\n",
    "                    break\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "    if res[0] is None: #found rowid in tree, inserted there if not present\n",
    "        return\n",
    "    elif res[1] is None: #insert to end of page\n",
    "        page_num = res[0]\n",
    "        cell = index_create_cell(index_dtype, index_value, [rowid], False, left_child_page=None)\n",
    "        \n",
    "        #running low on space\n",
    "        if pages[page_num]['available_bytes']/PAGE_SIZE<0.5:\n",
    "            index_leaf_split_page(file_name, page_num, cell, index_dtype)\n",
    "            return\n",
    "        try:\n",
    "            page_insert_cell(file_name, page_num, cell)\n",
    "        except:\n",
    "            index_leaf_split_page(file_name, page_num, cell, index_dtype)\n",
    "    else:\n",
    "        page_num = res[0]\n",
    "        cell_position = res[1]\n",
    "        cell = index_create_cell(index_dtype, index_value, [rowid], False, left_child_page=None)\n",
    "        \n",
    "        #running low on space\n",
    "        if pages[page_num]['available_bytes']/PAGE_SIZE<0.5:\n",
    "            index_leaf_split_page(file_name, page_num, cell, index_dtype, middle=True, index2insert=cell_position)\n",
    "            return\n",
    "        try:\n",
    "            index_insert_cell_in_page_middle(file_name, page_num, cell, cell_position)\n",
    "        except:\n",
    "            index_leaf_split_page(file_name, page_num, cell, index_dtype, middle=True, index2insert=cell_position)\n",
    "\n",
    "            \n",
    "def index_interior_split_page(file_name, split_page_num, cell2insert, new_rightmost_page, middle=False, index2insert=None):\n",
    "    pages = read_all_pages_in_file(file_name)\n",
    "    values = pages[split_page_num]\n",
    "\n",
    "    table_name = file_name[:-4]\n",
    "    parent_num = values['parent_page']\n",
    "    is_interior = not values['is_leaf']\n",
    "    is_leaf = values['is_leaf']\n",
    "    is_table = values['is_table']\n",
    "    assert(not is_table)\n",
    "    assert(is_interior)\n",
    "\n",
    "    num_cells = values['num_cells']\n",
    "    cells = values['cells']\n",
    "    middle_cell = int((num_cells+1)//2) #have to add one since we havent actually added the cell\n",
    "    if middle:\n",
    "        if index2insert > middle_cell:\n",
    "            to_left=False\n",
    "        else:\n",
    "            to_left=True\n",
    "            middle_cell-=1\n",
    "    \n",
    "    \n",
    "    middle_cell_binary = cells[middle_cell]['cell_binary']     \n",
    "    middle_index = cells[middle_cell]['index_value']\n",
    "    rightmost_child_page_right = new_rightmost_page\n",
    "    rightmost_child_page_left = cells[middle_cell]['left_child_page']\n",
    "\n",
    "    \n",
    "\n",
    "    if parent_num==-1: #ROOT CONDITION #children will also be interior nodes\n",
    "        rchild_num = write_new_page(table_name, is_table, is_interior, new_rightmost_page, split_page_num)\n",
    "        lchild_num = write_new_page(table_name, is_table, is_interior, rightmost_child_page_left, split_page_num)\n",
    "\n",
    "        for i in range(middle_cell): #Copy cells into left child\n",
    "            cell = cells[i]['cell_binary']\n",
    "            update_page_header(file_name, cells[i]['left_child_page'], parent=lchild_num) #update child to point header to lchild\n",
    "            page_insert_cell(file_name, lchild_num, cell)\n",
    "        update_page_header(file_name, rightmost_child_page_left, parent=lchild_num)#update parent of rightmost\n",
    "\n",
    "        for i in range(middle_cell, num_cells): #Copy cells into right child   #splitting the interior nodes,  the middle cell is not redundant in children\n",
    "            cell = cells[i]['cell_binary']\n",
    "            update_page_header(file_name, cells[i]['left_child_page'], parent=rchild_num)\n",
    "            page_insert_cell(file_name, rchild_num, cell)\n",
    "        update_page_header(file_name, rightmost_child_page_right, parent=rchild_num)\n",
    "\n",
    "        if middle: #insert cell to one of children\n",
    "            if not to_left:\n",
    "                index_insert_cell_in_page_middle(file_name, rchild_num, cell2insert, index2insert-middle_cell)\n",
    "            elif index2insert == middle_cell:\n",
    "                page_insert_cell(file_name, lchild_num, cell2insert)\n",
    "            else:\n",
    "                 index_insert_cell_in_page_middle(file_name, lchild_num, cell2insert, index2insert)\n",
    "        else: #insert cell to end of right child\n",
    "            page_insert_cell(file_name, rchild_num, cell2insert)\n",
    "        page_delete_cell(file_name, rchild_num, 0)\n",
    "\n",
    "        for i in range(num_cells):#deletes all cells in root\n",
    "            page_delete_cell(file_name, split_page_num, 0)\n",
    "\n",
    "        if middle and not to_left and index2insert == middle_cell: #cell inserted first entry in right child (goes up)\n",
    "            middle_cell_binary = index_read_cell(cell2insert, True)['cell_binary']\n",
    "            \n",
    "        page_insert_cell(file_name, split_page_num, middle_cell_binary)\n",
    "        update_page_header(file_name, split_page_num, rsibling_rchild=rchild_num)\n",
    "        update_cell_lpointer(file_name, split_page_num, 0, lchild_num)\n",
    "        return rchild_num  #return so we can update headers of pages that couldnt fit in the old page\n",
    "\n",
    "    else:\n",
    "        rsibling = write_new_page(table_name, is_table, is_interior, new_rightmost_page, parent_num)\n",
    "\n",
    "        for i in range(middle_cell+1, num_cells): #Copy cells into right sibling\n",
    "            cell = cells[i]['cell_binary']\n",
    "            update_page_header(file_name, cells[i]['left_child_page'], parent=rsibling)\n",
    "            page_insert_cell(file_name, rsibling, cell)\n",
    "\n",
    "        #delete all the copied cells from left sibling\n",
    "        j=middle_cell\n",
    "        for i in range(middle_cell, num_cells):\n",
    "            page_delete_cell(file_name, split_page_num, j)\n",
    "        update_page_header(file_name, split_page_num, rsibling_rchild=rightmost_child_page_left)\n",
    "\n",
    "        if middle:\n",
    "            if not to_left: #insert cell in ner right sibling\n",
    "                index_insert_cell_in_page_middle(file_name, rsibling, cell2insert, index2insert-middle_cell)\n",
    "            elif index2insert == middle_cell: #insert cell to end of left\n",
    "                page_insert_cell(file_name, split_page_num, cell2insert)\n",
    "            else:#insert cell to middle of left\n",
    "                 index_insert_cell_in_page_middle(file_name, rsibling, cell2insert, index2insert)\n",
    "        else: #insert cell to end of right\n",
    "            page_insert_cell(file_name, rsibling, cell2insert)\n",
    "        page_delete_cell(file_name, rsibling, 0)\n",
    "\n",
    "        if pages[parent_num]['rightmost_child_page']==split_page_num:\n",
    "            update_page_header(file_name, parent_num, rsibling_rchild=rsibling)\n",
    "\n",
    "        parent_page = read_cells_in_page(file_bytes, parent_num)\n",
    "        parent_cells = parent_page['cells']\n",
    "        \n",
    "        if parent_page[\"rightmost_child_page\"]==split_page_num:\n",
    "            update_page_header(file_name, parent_num, rsibling_rchild=rsibling)\n",
    "        \n",
    "        \n",
    "        for i, cell in enumerate(parent_cells):\n",
    "            if cell['index_value'] >  middle_index:\n",
    "                parent_index = i\n",
    "                update_cell_lpointer(file_name, parent_num, i, rsibling)\n",
    "            elif i==len(parent_cells)-1:\n",
    "                parent_index = None\n",
    "\n",
    "        if parent_index is None:\n",
    "            if parent_page['available_bytes']/PAGE_SIZE<0.5:\n",
    "                new_parent = index_interior_split_page(file_name, parent_num, middle_cell_binary, rsibling)\n",
    "                update_page_header(file_name, rsibling, parent = new_parent)\n",
    "                update_page_header(file_name, split_page_num, parent = new_parent)\n",
    "                return rsibling\n",
    "            try:\n",
    "                page_insert_cell(file_name, parent_num, middle_cell_binary)\n",
    "            except:\n",
    "                new_parent = index_interior_split_page(file_name, parent_num, middle_cell_binary, rsibling)\n",
    "                update_page_header(file_name, rsibling, parent = new_parent)\n",
    "                update_page_header(file_name, split_page_num, parent = new_parent)\n",
    "        else:\n",
    "            if parent_page['available_bytes']/PAGE_SIZE<0.5:\n",
    "                new_parent = index_interior_split_page(file_name, parent_num, middle_cell_binary, rsibling, middle=True, index2insert=parent_index)\n",
    "                update_page_header(file_name, rsibling, parent = new_parent)\n",
    "                update_page_header(file_name, split_page_num, parent = new_parent)\n",
    "                return rsibling\n",
    "            try:\n",
    "                index_insert_cell_in_page_middle(file_name, parent_num, middle_cell_binary, parent_index)\n",
    "            except:\n",
    "                new_parent = index_interior_split_page(file_name, parent_num, middle_cell_binary, rsibling, middle=True, index2insert=parent_index)\n",
    "                update_page_header(file_name, rsibling, parent = new_parent)\n",
    "                update_page_header(file_name, split_page_num, parent = new_parent)\n",
    "        return rsibling\n",
    "\n",
    "\n",
    "\n",
    "#could put these two together, but I dont care\n",
    "def index_leaf_split_page(file_name, split_page_num, cell2insert, index_dtype, middle=False, index2insert=None):\n",
    "    file_bytes = load_file(file_name)\n",
    "    values = read_cells_in_page(file_bytes, split_page_num)\n",
    "\n",
    "    table_name = file_name[:-4]\n",
    "    parent_num = values['parent_page']\n",
    "    is_interior = not values['is_leaf']\n",
    "    is_leaf = values['is_leaf']\n",
    "    is_table = values['is_table']\n",
    "    assert(not is_table)\n",
    "    assert(is_leaf)\n",
    "\n",
    "    num_cells = values['num_cells']\n",
    "    cells = values['cells']\n",
    "    middle_cell = int((num_cells+1)/2) #have to add one since we havent actually added the cell\n",
    "    if middle:\n",
    "        if index2insert > middle_cell:\n",
    "            to_left=False\n",
    "        else:\n",
    "            to_left=True\n",
    "            middle_cell-=1\n",
    "            \n",
    "    middle_cell_binary = cells[middle_cell]['cell_binary']\n",
    "    middle_index = cells[middle_cell]['index_value']\n",
    "    right_sibling_page = values['right_sibling_page']\n",
    "\n",
    "    if parent_num==-1: #IS ROOT ->create two children\n",
    "        rchild_num = write_new_page(table_name, is_table, False, -1, split_page_num)\n",
    "        lchild_num = write_new_page(table_name, is_table, False, rchild_num, split_page_num)\n",
    "\n",
    "        for i in range(middle_cell):   #Copy cells into left child\n",
    "            cell = cells[i]['cell_binary']\n",
    "            page_insert_cell(file_name, lchild_num, cell)\n",
    "\n",
    "        for i in range(middle_cell, num_cells): #Copy cells into right child\n",
    "            cell = cells[i]['cell_binary']\n",
    "            page_insert_cell(file_name, rchild_num, cell)\n",
    "\n",
    "        if middle:\n",
    "            if not to_left:\n",
    "                index_insert_cell_in_page_middle(file_name, rchild_num, cell2insert, index2insert-middle_cell)\n",
    "            elif index2insert == middle_cell:\n",
    "                page_insert_cell(file_name, lchild_num, cell2insert)\n",
    "            else:\n",
    "                 index_insert_cell_in_page_middle(file_name, lchild_num, cell2insert, index2insert)\n",
    "        else:\n",
    "            page_insert_cell(file_name, rchild_num, cell2insert)\n",
    "        page_delete_cell(file_name, rchild_num, 0)\n",
    "\n",
    "        for i in range(num_cells):#deletes all cells in root\n",
    "            page_delete_cell(file_name, split_page_num, 0)\n",
    "\n",
    "        if middle and not to_left and index2insert == middle_cell: #cell inseted first entry in right child (goes up)\n",
    "            middle_cell_binary = index_read_cell(cell2insert, False)['cell_binary']\n",
    "            middle_cell_binary = struct.pack(endian+'i', lchild_num) + middle_cell_binary\n",
    "        else:\n",
    "            middle_cell_binary = struct.pack(endian+'i', lchild_num) + middle_cell_binary\n",
    "        update_page_header(file_name, split_page_num, rsibling_rchild=rchild_num, is_interior=True)\n",
    "        page_insert_cell(file_name, split_page_num, middle_cell_binary)\n",
    "\n",
    "\n",
    "\n",
    "    else: #Non-root ->propagate upward\n",
    "        rsibling = write_new_page(table_name, is_table, is_interior, right_sibling_page, parent_num)\n",
    "        update_page_header(file_name, split_page_num, rsibling_rchild=rsibling)\n",
    "        \n",
    "        for i in range(middle_cell, num_cells): #Copy cells into right sibling\n",
    "            cell = cells[i]['cell_binary']\n",
    "            page_insert_cell(file_name, rsibling, cell)\n",
    "\n",
    "\n",
    "        j=middle_cell\n",
    "        for i in range(middle_cell, num_cells):\n",
    "            page_delete_cell(file_name, split_page_num, j)\n",
    "\n",
    "        if middle:\n",
    "            \n",
    "            if not to_left:\n",
    "                index_insert_cell_in_page_middle(file_name, rsibling, cell2insert, index2insert-middle_cell)\n",
    "            elif index2insert == middle_cell:\n",
    "                page_insert_cell(file_name, split_page_num, cell2insert)\n",
    "            else:\n",
    "                 index_insert_cell_in_page_middle(file_name, rsibling, cell2insert, index2insert)\n",
    "        else:\n",
    "            page_insert_cell(file_name, rsibling, cell2insert)\n",
    "        page_delete_cell(file_name, rsibling, 0)\n",
    "\n",
    "        if middle and not to_left and index2insert == middle_cell: #cell inseted first entry in right child (goes up)\n",
    "            middle_index = index_read_cell(cell2insert, False)['index_value']\n",
    "            middle_cell_binary = index_read_cell(cell2insert, False)['cell_binary']\n",
    "        middle_cell_binary = struct.pack(endian+'i', split_page_num) + middle_cell_binary\n",
    "        \n",
    "        parent_page = read_cells_in_page(file_bytes, parent_num)\n",
    "        parent_cells = parent_page['cells']\n",
    "        \n",
    "        if parent_page[\"rightmost_child_page\"]==split_page_num:\n",
    "            update_page_header(file_name, parent_num, rsibling_rchild=rsibling)\n",
    "        \n",
    "        \n",
    "        for i, cell in enumerate(parent_cells):\n",
    "            if cell['index_value'] >  middle_index:\n",
    "                parent_index = i\n",
    "                update_cell_lpointer(file_name, parent_num, i, rsibling)\n",
    "                break\n",
    "            elif i==len(parent_cells)-1:\n",
    "                parent_index = None\n",
    "        \n",
    "        if parent_index is None:\n",
    "            if parent_page['available_bytes']/PAGE_SIZE<0.5:\n",
    "                    new_parent = index_interior_split_page(file_name, parent_num, middle_cell_binary, rsibling)\n",
    "                    update_page_header(file_name, rsibling, parent = new_parent)\n",
    "                    update_page_header(file_name, split_page_num, parent = new_parent)\n",
    "                    return None\n",
    "            try:\n",
    "                page_insert_cell(file_name, parent_num, middle_cell_binary)\n",
    "            except:\n",
    "                new_parent = index_interior_split_page(file_name, parent_num, middle_cell_binary, rsibling)\n",
    "                update_page_header(file_name, rsibling, parent = new_parent)\n",
    "                update_page_header(file_name, split_page_num, parent = new_parent)\n",
    "        else:\n",
    "            \n",
    "            if parent_page['available_bytes']/PAGE_SIZE<0.5:\n",
    "                new_parent = index_interior_split_page(file_name, parent_num, middle_cell_binary, rsibling, middle=True, index2insert=parent_index)\n",
    "                update_page_header(file_name, rsibling, parent = new_parent)\n",
    "                update_page_header(file_name, split_page_num, parent = new_parent)\n",
    "                return None\n",
    "            try:\n",
    "                index_insert_cell_in_page_middle(file_name, parent_num, middle_cell_binary, parent_index)\n",
    "            except:\n",
    "                new_parent = index_interior_split_page(file_name, parent_num, middle_cell_binary, rsibling, middle=True, index2insert=parent_index)\n",
    "                update_page_header(file_name, rsibling, parent = new_parent)\n",
    "                update_page_header(file_name, split_page_num, parent = new_parent)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T22:27:31.915617Z",
     "start_time": "2019-12-01T22:27:31.697795Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "table_name_column1\n",
      "\n",
      "page_number:  0\n",
      "parent_page:  -1\n",
      "right_child_page:  6\n",
      "bytes remaining: 362\n",
      "indx_val:  75 left child:  2\n",
      "indx_val:  218 left child:  7\n",
      "indx_val:  308 left child:  4\n",
      "indx_val:  406 left child:  8\n",
      "indx_val:  559 left child:  1\n",
      "indx_val:  715 left child:  5\n",
      "indx_val:  856 left child:  3\n",
      "\n",
      "page_number:  1\n",
      "parent_page:  0\n",
      "right_sibling_page:  5\n",
      "bytes remaining: 286\n",
      "[416, 421, 423, 427, 430, 441, 443, 444, 450, 459, 467, 496, 499, 531, 532]\n",
      "\n",
      "page_number:  2\n",
      "parent_page:  0\n",
      "right_sibling_page:  7\n",
      "bytes remaining: 356\n",
      "[3, 8, 15, 37, 40, 50, 57, 60, 68, 73]\n",
      "\n",
      "page_number:  3\n",
      "parent_page:  0\n",
      "right_sibling_page:  6\n",
      "bytes remaining: 286\n",
      "[719, 732, 734, 736, 755, 782, 806, 819, 824, 464, 836, 837, 839, 840, 847]\n",
      "\n",
      "page_number:  4\n",
      "parent_page:  0\n",
      "right_sibling_page:  8\n",
      "bytes remaining: 384\n",
      "[224, 226, 241, 254, 257, 278, 298, 303]\n",
      "\n",
      "page_number:  5\n",
      "parent_page:  0\n",
      "right_sibling_page:  3\n",
      "bytes remaining: 342\n",
      "[592, 593, 613, 650, 658, 669, 673, 674, 684, 699, 703]\n",
      "\n",
      "page_number:  6\n",
      "parent_page:  0\n",
      "right_sibling_page:  -1\n",
      "bytes remaining: 328\n",
      "[859, 867, 884, 901, 924, 936, 939, 958, 961, 991, 993, 994]\n",
      "\n",
      "page_number:  7\n",
      "parent_page:  0\n",
      "right_sibling_page:  4\n",
      "bytes remaining: 356\n",
      "[82, 88, 103, 123, 143, 145, 156, 175, 188, 200]\n",
      "\n",
      "page_number:  8\n",
      "parent_page:  0\n",
      "right_sibling_page:  1\n",
      "bytes remaining: 356\n",
      "[312, 318, 328, 336, 260, 349, 221, 380, 383, 402]\n"
     ]
    }
   ],
   "source": [
    "initialize_file(\"table_name_column1\", False)\n",
    "\n",
    "for i in range(100):\n",
    "    index_insert(\"table_name\", \"column1\", \"INT\", randint(0,1000), i)\n",
    "print_it(\"table_name_column1.ndx\", page_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T22:02:19.411109Z",
     "start_time": "2019-12-01T22:02:19.401156Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "table_name_column1\n",
      "\n",
      "page_number:  0\n",
      "parent_page:  -1\n",
      "right_child_page:  6\n",
      "bytes remaining: 388\n",
      "indx_val:  220 left child:  2\n",
      "indx_val:  330 left child:  4\n",
      "indx_val:  422 left child:  7\n",
      "indx_val:  573 left child:  1\n",
      "indx_val:  816 left child:  3\n",
      "indx_val:  892 left child:  5\n",
      "\n",
      "page_number:  1\n",
      "parent_page:  0\n",
      "right_sibling_page:  3\n",
      "bytes remaining: 286\n",
      "[450, 454, 462, 479, 511, 512, 528, 532, 534, 537, 544, 549, 550, 552, 564]\n",
      "\n",
      "page_number:  2\n",
      "parent_page:  0\n",
      "right_sibling_page:  4\n",
      "bytes remaining: 264\n",
      "[9, 15, 70, 72, 79, 98, 102, 103, 118, 122, 124, 131, 136, 137, 186, 199]\n",
      "\n",
      "page_number:  3\n",
      "parent_page:  0\n",
      "right_sibling_page:  5\n",
      "bytes remaining: 272\n",
      "[581, 595, 607, 616, 619, 631, 681, 529, 725, 726, 735, 744, 750, 753, 755, 774]\n",
      "\n",
      "page_number:  4\n",
      "parent_page:  0\n",
      "right_sibling_page:  7\n",
      "bytes remaining: 380\n",
      "[232, 253, 262, 273, 277, 281, 290, 329]\n",
      "\n",
      "page_number:  5\n",
      "parent_page:  0\n",
      "right_sibling_page:  6\n",
      "bytes remaining: 338\n",
      "[819, 821, 825, 826, 828, 832, 844, 847, 848, 882, 888]\n",
      "\n",
      "page_number:  6\n",
      "parent_page:  0\n",
      "right_sibling_page:  -1\n",
      "bytes remaining: 314\n",
      "[920, 929, 931, 932, 938, 940, 942, 949, 982, 983, 987, 990, 992]\n",
      "\n",
      "page_number:  7\n",
      "parent_page:  0\n",
      "right_sibling_page:  1\n",
      "bytes remaining: 352\n",
      "[331, 346, 271, 360, 362, 363, 385, 395, 407, 413]\n"
     ]
    }
   ],
   "source": [
    "#cell = index_create_cell('int',4,[0],False)\n",
    "#index_insert_cell_in_page_middle(\"table_name_column1.ndx\", 0,cell, 3)\n",
    "\n",
    "print_it(\"table_name_column1.ndx\", page_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_cell_indx_given_index_value(file_name, index_value):\n",
    "    page_num=0\n",
    "    pages = read_all_pages_in_file(file_name)\n",
    "    return get_page_cell_indx(pages, rowid, page_num)\n",
    "\n",
    "\n",
    "def get_page_cell_indx(pages, value, page_num):\n",
    "    is_table= pages[page_num]['is_table']\n",
    "    is_leaf = pages[page_num]['is_leaf']\n",
    "    assert(is_table)\n",
    "    for cell_indx, cell in enumerate(pages[page_num]['cells']):\n",
    "        if (cell['rowid'] == value and is_leaf): #got a match\n",
    "            return page_num, cell_indx\n",
    "\n",
    "        elif (cell['rowid'] > value and not is_leaf): #same\n",
    "            page_num = cell['left_child_page']\n",
    "            return get_page_cell_indx(pages, value, page_num)\n",
    "\n",
    "        elif (cell['rowid']<=value and not is_leaf):\n",
    "            page_num = pages[page_num]['rightmost_child_page']\n",
    "            return get_page_cell_indx(pages, value, page_num)\n",
    "        else:\n",
    "            pass\n",
    "    if is_leaf: #No match and is leaf node\n",
    "        return page_num, None\n",
    "    else:\n",
    "        assert(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_insert_cell_in_page_middle(file_name, page_num, cell, cell_position):\n",
    "    file_bytes = load_file(file_name)\n",
    "    page = load_page(file_bytes, page_num)\n",
    "    page = bytearray(page)\n",
    "\n",
    "    num_cells = struct.unpack(endian+'h', page[2:4])[0]\n",
    "    assert(cell_indx<=num_cells-1)#index starts at 0\n",
    "    assert(cell_indx>=0)\n",
    "    assert(len(cell)<page_available_bytes(file_bytes, page_num))\n",
    "\n",
    "    cell_content_area_start = struct.unpack(endian+'h', page[4:6])[0]\n",
    "    end_of_array = 16+2*num_cells\n",
    "    array_idx_top = 16+2*cell_indx\n",
    "    array_idx_bot = 16+2*(cell_indx+1)\n",
    "\n",
    "    cell_top_loc, cell_bot_loc = get_cell_indices(page, cell_indx)\n",
    "    dis2move= len(cell)\n",
    "    #shift cell content down\n",
    "    page = shift_page_content(page, cell_content_area_start, cell_bot_loc, dis2move, up=True)\n",
    "    #since we just shifted every cell, every value in cell_array is off\n",
    "    page = update_array_values(page, cell_indx, num_cells, dis2move, up=True)\n",
    "    #change the cell_start area\n",
    "    page[4:6] = struct.pack(endian+'h', cell_content_area_start-dis2move)\n",
    "    #shift cell array up (deletes entry for deleted cell)\n",
    "    page = shift_page_content(page, array_idx_top, end_of_array, 2, up=False)\n",
    "    page[array_idx_top:array_idx_bot] = struct.pack(endian+'h', cell_bot_loc-dis2move)\n",
    "    page[cell_bot_loc-dis2move:cell_bot_loc] = cell\n",
    "    #update num of cells\n",
    "    page[2:4] = struct.pack(endian+'h', num_cells+1)\n",
    "    assert(len(page)==PAGE_SIZE)\n",
    "    save_page(file_name, page_num, page)\n",
    "    #ensure page is same size\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_rowid_to_cell(file_name, page_num, cell_indx, rowid, cell):\n",
    "    cell_binary = cell['cell_binary']+struct.pack(endian+'i', rowid)\n",
    "    try:\n",
    "        page_update_cell(file_name, page_num, cell_indx, cell_binary)\n",
    "    except:\n",
    "        return\n",
    "\n",
    "#############################################################################\n",
    "#IN PROGRESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rowid',\n",
       " 'table_name',\n",
       " 'column_name',\n",
       " 'data_type',\n",
       " 'ordinal_position',\n",
       " 'is_nullable']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_column_names_from_catalog(\"davisbase_columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "def get_operator_fn(op):\n",
    "    return {\n",
    "    '=' : operator.eq,\n",
    "    '<' : operator.lt,\n",
    "    '>' : operator.gt,\n",
    "    '>=' : operator.ge,\n",
    "    '<=' : operator.le,\n",
    "    }[op]        \n",
    "\n",
    "def query(command: str):\n",
    "    '''\n",
    "    command : Select statement eg. select a.a,b.b,c from a,b where a.a = b.a;\n",
    "    return : None\n",
    "    '''\n",
    "    print(\"User wants to query {}\".format(command))\n",
    "    ## check if the select statement is correct or not\n",
    "    operator_list = ['=','>','<','>=','<=']\n",
    "    \n",
    "    query_match = \"select\\s+(.*?)\\s*(?i)from\\s+(.*?)\\s*((?i)where\\s(.*?)\\s*)?;\"\n",
    "    if re.match(query_match, command):\n",
    "        stmt = sqlparse.parse(command)[0]\n",
    "        where_clause = str(stmt.tokens[-1])\n",
    "        where_clause = re.sub(\"\\s\", \"\", re.split(';',re.sub(\"(?i)where\",\"\",where_clause))[0])\n",
    "        res = [i for i in operator_list if where_clause.find(i)!=-1]\n",
    "        where_clause = re.split('=|>|<|>=|<=|\\s',where_clause)\n",
    "        tablename = str(stmt.tokens[-3]).split(\",\")[0]\n",
    "        columns = str(stmt.tokens[2]).split(\",\")\n",
    "#         print(where_clause,\"\\t\",tablename,\"\\t\",columns)\n",
    "        return str(where_clause[0]),str(where_clause[1]),res[0], tablename, columns\n",
    "    else:\n",
    "        \n",
    "        return -1,-1,-1,-1,-1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "operator.eq(\"a\",'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_from(SQL):\n",
    "    \n",
    "    where_op, where_value, oper, table_name, columns =  query(SQL)\n",
    "    \n",
    "    column_list = get_column_names_from_catalog(table_name)\n",
    "    \n",
    "    index = column_list.index(where_op)\n",
    "        \n",
    "    if where_op == -1:\n",
    "        print(\"Enter correct query\")\n",
    "        \n",
    "    flag = False\n",



    "    for node in read_all_pages_in_file(table_name+\".tbl\"):\n",

    "        if node['is_leaf'] :\n",
    "            for cell in node['cells']:\n",
    "                data = cell['data']\n",
    "                if index == 0 :\n",
    "                    op1 = cell['rowid']\n",
    "                    op2 = where_value\n",
    "                else:\n",
    "                    op1 = where_value\n",
    "                    op2 = \"'\"+str(data[index - 1]) + \"'\"\n",
    "                \n",
    "                if get_operator_fn(oper)(op1, op2):\n",
    "                    print(cell)\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_all_pages_in_file(table_name+\".tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,

   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User wants to query select * from davisbase_columns where table_name = 'davisbase_tables';\n",
      "{'bytes': 45, 'rowid': 1, 'data': ['davisbase_tables', 'rowid', 'INT', 1, 'NO', 'NO', 'NO'], 'cell_size': 45, 'cell_binary': b\"'\\x00\\x01\\x00\\x00\\x00\\x07\\x1c\\x11\\x0f\\x01\\x0e\\x0e\\x0edavisbase_tablesrowidINT\\x01NONONO\"}\n"
     ]
    }
   ],
   "source": [
    "import sqlparse\n",
    "select_from(\"select * from davisbase_columns where table_name = 'davisbase_tables';\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': 0,\n",
       "  'parent_page': -1,\n",
       "  'is_table': True,\n",
       "  'is_leaf': False,\n",
       "  'num_cells': 1,\n",
       "  'available_bytes': 486,\n",
       "  'rightmost_child_page': 1,\n",
       "  'cells': [{'left_child_page': 2,\n",
       "    'rowid': 6,\n",
       "    'cell_size': 8,\n",
       "    'cell_binary': b'\\x02\\x00\\x00\\x00\\x06\\x00\\x00\\x00'}]},\n",
       " {'page_number': 1,\n",
       "  'parent_page': 0,\n",
       "  'is_table': True,\n",
       "  'is_leaf': True,\n",
       "  'num_cells': 5,\n",
       "  'available_bytes': 220,\n",
       "  'right_sibling_page': -1,\n",
       "  'cells': [{'bytes': 51,\n",
       "    'rowid': 6,\n",
       "    'data': ['davisbase_columns', 'data_type', 'TEXT', 4, 'NO', 'NO', 'NO'],\n",
       "    'cell_size': 51,\n",
       "    'cell_binary': b'-\\x00\\x06\\x00\\x00\\x00\\x07\\x1d\\x15\\x10\\x01\\x0e\\x0e\\x0edavisbase_columnsdata_typeTEXT\\x04NONONO'},\n",
       "   {'bytes': 61,\n",
       "    'rowid': 7,\n",
       "    'data': ['davisbase_columns',\n",
       "     'ordinal_position',\n",
       "     'TINYINT',\n",
       "     5,\n",
       "     'NO',\n",
       "     'NO',\n",
       "     'NO'],\n",
       "    'cell_size': 61,\n",
       "    'cell_binary': b'7\\x00\\x07\\x00\\x00\\x00\\x07\\x1d\\x1c\\x13\\x01\\x0e\\x0e\\x0edavisbase_columnsordinal_positionTINYINT\\x05NONONO'},\n",
       "   {'bytes': 53,\n",
       "    'rowid': 8,\n",
       "    'data': ['davisbase_columns', 'is_nullable', 'TEXT', 6, 'NO', 'NO', 'NO'],\n",
       "    'cell_size': 53,\n",
       "    'cell_binary': b'/\\x00\\x08\\x00\\x00\\x00\\x07\\x1d\\x17\\x10\\x01\\x0e\\x0e\\x0edavisbase_columnsis_nullableTEXT\\x06NONONO'},\n",
       "   {'bytes': 48,\n",
       "    'rowid': 9,\n",
       "    'data': ['davisbase_columns', 'unique', 'TEXT', 7, 'NO', 'NO', 'NO'],\n",
       "    'cell_size': 48,\n",
       "    'cell_binary': b'*\\x00\\t\\x00\\x00\\x00\\x07\\x1d\\x12\\x10\\x01\\x0e\\x0e\\x0edavisbase_columnsuniqueTEXT\\x07NONONO'},\n",
       "   {'bytes': 53,\n",
       "    'rowid': 10,\n",
       "    'data': ['davisbase_columns', 'primary_key', 'TEXT', 8, 'NO', 'NO', 'NO'],\n",
       "    'cell_size': 53,\n",
       "    'cell_binary': b'/\\x00\\n\\x00\\x00\\x00\\x07\\x1d\\x17\\x10\\x01\\x0e\\x0e\\x0edavisbase_columnsprimary_keyTEXT\\x08NONONO'}]},\n",
       " {'page_number': 2,\n",
       "  'parent_page': 0,\n",
       "  'is_table': True,\n",
       "  'is_leaf': True,\n",
       "  'num_cells': 5,\n",
       "  'available_bytes': 239,\n",
       "  'right_sibling_page': 1,\n",
       "  'cells': [{'bytes': 45,\n",
       "    'rowid': 1,\n",
       "    'data': ['davisbase_tables', 'rowid', 'INT', 1, 'NO', 'NO', 'NO'],\n",
       "    'cell_size': 45,\n",
       "    'cell_binary': b\"'\\x00\\x01\\x00\\x00\\x00\\x07\\x1c\\x11\\x0f\\x01\\x0e\\x0e\\x0edavisbase_tablesrowidINT\\x01NONONO\"},\n",
       "   {'bytes': 51,\n",
       "    'rowid': 2,\n",
       "    'data': ['davisbase_tables', 'table_name', 'TEXT', 2, 'NO', 'NO', 'NO'],\n",
       "    'cell_size': 51,\n",
       "    'cell_binary': b'-\\x00\\x02\\x00\\x00\\x00\\x07\\x1c\\x16\\x10\\x01\\x0e\\x0e\\x0edavisbase_tablestable_nameTEXT\\x02NONONO'},\n",
       "   {'bytes': 46,\n",
       "    'rowid': 3,\n",
       "    'data': ['davisbase_columns', 'rowid', 'INT', 1, 'NO', 'NO', 'NO'],\n",
       "    'cell_size': 46,\n",
       "    'cell_binary': b'(\\x00\\x03\\x00\\x00\\x00\\x07\\x1d\\x11\\x0f\\x01\\x0e\\x0e\\x0edavisbase_columnsrowidINT\\x01NONONO'},\n",
       "   {'bytes': 52,\n",
       "    'rowid': 4,\n",
       "    'data': ['davisbase_columns', 'table_name', 'TEXT', 2, 'NO', 'NO', 'NO'],\n",
       "    'cell_size': 52,\n",
       "    'cell_binary': b'.\\x00\\x04\\x00\\x00\\x00\\x07\\x1d\\x16\\x10\\x01\\x0e\\x0e\\x0edavisbase_columnstable_nameTEXT\\x02NONONO'},\n",
       "   {'bytes': 53,\n",
       "    'rowid': 5,\n",
       "    'data': ['davisbase_columns', 'column_name', 'TEXT', 3, 'NO', 'NO', 'NO'],\n",
       "    'cell_size': 53,\n",
       "    'cell_binary': b'/\\x00\\x05\\x00\\x00\\x00\\x07\\x1d\\x17\\x10\\x01\\x0e\\x0e\\x0edavisbase_columnscolumn_nameTEXT\\x03NONONO'}]}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_all_pages_in_file(\"davisbase_columns.tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
